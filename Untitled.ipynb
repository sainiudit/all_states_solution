{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'cat1' 'cat2' 'cat3' 'cat4' 'cat5' 'cat6' 'cat7' 'cat8' 'cat9'\n",
      " 'cat10' 'cat11' 'cat12' 'cat13' 'cat14' 'cat15' 'cat16' 'cat17' 'cat18'\n",
      " 'cat19' 'cat20' 'cat21' 'cat22' 'cat23' 'cat24' 'cat25' 'cat26' 'cat27'\n",
      " 'cat28' 'cat29' 'cat30' 'cat31' 'cat32' 'cat33' 'cat34' 'cat35' 'cat36'\n",
      " 'cat37' 'cat38' 'cat39' 'cat40' 'cat41' 'cat42' 'cat43' 'cat44' 'cat45'\n",
      " 'cat46' 'cat47' 'cat48' 'cat49' 'cat50' 'cat51' 'cat52' 'cat53' 'cat54'\n",
      " 'cat55' 'cat56' 'cat57' 'cat58' 'cat59' 'cat60' 'cat61' 'cat62' 'cat63'\n",
      " 'cat64' 'cat65' 'cat66' 'cat67' 'cat68' 'cat69' 'cat70' 'cat71' 'cat72'\n",
      " 'cat73' 'cat74' 'cat75' 'cat76' 'cat77' 'cat78' 'cat79' 'cat80' 'cat81'\n",
      " 'cat82' 'cat83' 'cat84' 'cat85' 'cat86' 'cat87' 'cat88' 'cat89' 'cat90'\n",
      " 'cat91' 'cat92' 'cat93' 'cat94' 'cat95' 'cat96' 'cat97' 'cat98' 'cat99'\n",
      " 'cat100' 'cat101' 'cat102' 'cat103' 'cat104' 'cat105' 'cat106' 'cat107'\n",
      " 'cat108' 'cat109' 'cat110' 'cat111' 'cat112' 'cat113' 'cat114' 'cat115'\n",
      " 'cat116' 'cont1' 'cont2' 'cont3' 'cont4' 'cont5' 'cont6' 'cont7' 'cont8'\n",
      " 'cont9' 'cont10' 'cont11' 'cont12' 'cont13' 'cont14' 'loss']\n",
      "cat89\n",
      "cat92\n",
      "cat96\n",
      "cat99\n",
      "cat103\n",
      "cat106\n",
      "cat109\n",
      "cat110\n",
      "cat111\n",
      "cat113\n",
      "cat116\n",
      "['cont1' 'cont2' 'cont3' 'cont4' 'cont5' 'cont6' 'cont7' 'cont8' 'cont9'\n",
      " 'cont10' 'cont11' 'cont12' 'cont13' 'cont14' 'cat1' 'cat2' 'cat3' 'cat4'\n",
      " 'cat5' 'cat6' 'cat7' 'cat8' 'cat9' 'cat10' 'cat11' 'cat12' 'cat13' 'cat14'\n",
      " 'cat15' 'cat16' 'cat17' 'cat18' 'cat19' 'cat20' 'cat21' 'cat22' 'cat23'\n",
      " 'cat24' 'cat25' 'cat26' 'cat27' 'cat28' 'cat29' 'cat30' 'cat31' 'cat32'\n",
      " 'cat33' 'cat34' 'cat35' 'cat36' 'cat37' 'cat38' 'cat39' 'cat40' 'cat41'\n",
      " 'cat42' 'cat43' 'cat44' 'cat45' 'cat46' 'cat47' 'cat48' 'cat49' 'cat50'\n",
      " 'cat51' 'cat52' 'cat53' 'cat54' 'cat55' 'cat56' 'cat57' 'cat58' 'cat59'\n",
      " 'cat60' 'cat61' 'cat62' 'cat63' 'cat64' 'cat65' 'cat66' 'cat67' 'cat68'\n",
      " 'cat69' 'cat70' 'cat71' 'cat72' 'cat73' 'cat74' 'cat75' 'cat76' 'cat77'\n",
      " 'cat78' 'cat79' 'cat80' 'cat81' 'cat82' 'cat83' 'cat84' 'cat85' 'cat86'\n",
      " 'cat87' 'cat88' 'cat90' 'cat91' 'cat93' 'cat94' 'cat95' 'cat97' 'cat98'\n",
      " 'cat100' 'cat101' 'cat102' 'cat104' 'cat105' 'cat107' 'cat108' 'cat112'\n",
      " 'cat114' 'cat115']\n",
      "cat1\n",
      "cat2\n",
      "cat3\n",
      "cat4\n",
      "cat5\n",
      "cat6\n",
      "cat7\n",
      "cat8\n",
      "cat9\n",
      "cat10\n",
      "cat11\n",
      "cat12\n",
      "cat13\n",
      "cat14\n",
      "cat15\n",
      "cat16\n",
      "cat17\n",
      "cat18\n",
      "cat19\n",
      "cat20\n",
      "cat21\n",
      "cat22\n",
      "cat23\n",
      "cat24\n",
      "cat25\n",
      "cat26\n",
      "cat27\n",
      "cat28\n",
      "cat29\n",
      "cat30\n",
      "cat31\n",
      "cat32\n",
      "cat33\n",
      "cat34\n",
      "cat35\n",
      "cat36\n",
      "cat37\n",
      "cat38\n",
      "cat39\n",
      "cat40\n",
      "cat41\n",
      "cat42\n",
      "cat43\n",
      "cat44\n",
      "cat45\n",
      "cat46\n",
      "cat47\n",
      "cat48\n",
      "cat49\n",
      "cat50\n",
      "cat51\n",
      "cat52\n",
      "cat53\n",
      "cat54\n",
      "cat55\n",
      "cat56\n",
      "cat57\n",
      "cat58\n",
      "cat59\n",
      "cat60\n",
      "cat61\n",
      "cat62\n",
      "cat63\n",
      "cat64\n",
      "cat65\n",
      "cat66\n",
      "cat67\n",
      "cat68\n",
      "cat69\n",
      "cat70\n",
      "cat71\n",
      "cat72\n",
      "cat73\n",
      "cat74\n",
      "cat75\n",
      "cat76\n",
      "cat77\n",
      "cat78\n",
      "cat79\n",
      "cat80\n",
      "cat81\n",
      "cat82\n",
      "cat83\n",
      "cat84\n",
      "cat85\n",
      "cat86\n",
      "cat87\n",
      "cat88\n",
      "cat90\n",
      "cat91\n",
      "cat93\n",
      "cat94\n",
      "cat95\n",
      "cat97\n",
      "cat98\n",
      "cat100\n",
      "cat101\n",
      "cat102\n",
      "cat104\n",
      "cat105\n",
      "cat107\n",
      "cat108\n",
      "cat112\n",
      "cat114\n",
      "cat115\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter min_impurity_split for estimator GradientBoostingRegressor. Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-66424841f360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'min_impurity_split'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'min_samples_split'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'min_samples_leaf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'subsample'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mdata_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    268\u001b[0m                                      \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                                      \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                                      (key, self.__class__.__name__))\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter min_impurity_split for estimator GradientBoostingRegressor. Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn import preprocessing \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas \n",
    "from sklearn import grid_search\n",
    "import os\n",
    "os.chdir(\"/home/udit/ipython/notebook/all/input\")\n",
    "from sklearn import cross_validation as cv\n",
    "\n",
    "data_train = pandas.read_csv(\"train.csv\")\n",
    "data_test = pandas.read_csv(\"test.csv\")\n",
    "predictors = data_train.columns.values \n",
    "print(predictors)\n",
    "categorical_predictors = predictors[1:-15]\n",
    "categorical_predictors_to_remove = []\n",
    "temp_categorical_predictors = [] \n",
    "for j in range(0,len(categorical_predictors)): \n",
    "    tester = True \n",
    "    i = categorical_predictors[j]\n",
    "    for x in data_test[i].unique(): \n",
    "        if x not in data_train[i].unique(): \n",
    "            tester = False\n",
    "    if tester == False: \n",
    "        print(i)\n",
    "        categorical_predictors_to_remove.append(i)\n",
    "for i in categorical_predictors: \n",
    "    if i not in categorical_predictors_to_remove: \n",
    "        temp_categorical_predictors.append(i)\n",
    "categorical_predictors = temp_categorical_predictors\n",
    "predictors = predictors[-15:-1]\n",
    "predictors = np.append(predictors,categorical_predictors)\n",
    "print(predictors)\n",
    "for i in categorical_predictors: \n",
    "    print(i)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(data_train[i])\n",
    "    data_train[i] = le.transform(data_train[i])\n",
    "    data_test[i] = le.transform(data_test[i])\n",
    "scaler = preprocessing.StandardScaler() \n",
    "scaler.fit(data_train[predictors])\n",
    "data_train[predictors] = scaler.transform(data_train[predictors])\n",
    "data_test[predictors] = scaler.transform(data_test[predictors])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1     7660500.8736            7.60m\n",
      "         2     7033685.9421            7.58m\n",
      "         3     6524896.4118            7.51m\n",
      "         4     6100972.9511            7.41m\n",
      "         5     5742007.8860            7.27m\n",
      "         6     5444161.7954            7.21m\n",
      "         7     5180516.3303            7.34m\n",
      "         8     4950054.0925            7.26m\n",
      "         9     4753378.1693            7.23m\n",
      "        10     4588151.2261            7.17m\n",
      "        11     4445205.1911            7.23m\n",
      "        12     4322967.9658            7.30m\n",
      "        13     4214195.7925            7.33m\n",
      "        14     4113923.9679            7.44m\n",
      "        15     4019496.4432            7.48m\n",
      "        16     3943085.2262            7.52m\n",
      "        17     3874101.3624            7.51m\n",
      "        18     3811434.7820            7.46m\n",
      "        19     3745417.6736            7.49m\n",
      "        20     3689979.4412            7.52m\n",
      "        21     3640259.6082            7.53m\n",
      "        22     3598765.2226            7.46m\n",
      "        23     3559991.8825            7.44m\n",
      "        24     3524993.1904            7.38m\n",
      "        25     3490854.1438            7.32m\n",
      "        26     3457132.3792            7.26m\n",
      "        27     3426443.2290            7.20m\n",
      "        28     3395650.5376            7.11m\n",
      "        29     3366185.4506            7.05m\n",
      "        30     3338130.6492            6.98m\n",
      "        31     3315575.1264            6.82m\n",
      "        32     3289940.6079            6.66m\n",
      "        33     3271184.3586            6.50m\n",
      "        34     3249728.7334            6.35m\n",
      "        35     3226111.2455            6.21m\n",
      "        36     3210930.5281            6.05m\n",
      "        37     3191551.5357            5.90m\n",
      "        38     3172011.7925            5.77m\n",
      "        39     3159336.4976            5.63m\n",
      "        40     3140271.9784            5.52m\n",
      "        41     3125136.6479            5.39m\n",
      "        42     3108731.3193            5.26m\n",
      "        43     3096165.4741            5.15m\n",
      "        44     3081466.9776            5.02m\n",
      "        45     3068334.7160            4.90m\n",
      "        46     3056900.3626            4.79m\n",
      "        47     3047502.4465            4.66m\n",
      "        48     3037530.4039            4.56m\n",
      "        49     3026017.5408            4.44m\n",
      "        50     3012697.7986            4.34m\n",
      "        51     3000570.4632            4.23m\n",
      "        52     2991015.5074            4.13m\n",
      "        53     2981975.3847            4.01m\n",
      "        54     2973679.2741            3.91m\n",
      "        55     2962758.3265            3.81m\n",
      "        56     2952643.1337            3.71m\n",
      "        57     2945140.4176            3.61m\n",
      "        58     2934771.6879            3.51m\n",
      "        59     2924427.1923            3.42m\n",
      "        60     2916081.6845            3.32m\n",
      "        61     2905694.9780            3.23m\n",
      "        62     2897657.9480            3.13m\n",
      "        63     2888582.7925            3.04m\n",
      "        64     2880730.8850            2.95m\n",
      "        65     2876404.4230            2.85m\n",
      "        66     2870288.9764            2.76m\n",
      "        67     2862397.8168            2.67m\n",
      "        68     2857535.0913            2.58m\n",
      "        69     2851382.3728            2.49m\n",
      "        70     2845381.5309            2.40m\n",
      "        71     2840399.9643            2.31m\n",
      "        72     2835321.4485            2.22m\n",
      "        73     2827888.2204            2.13m\n",
      "        74     2820082.8616            2.05m\n",
      "        75     2814315.1074            1.96m\n",
      "        76     2806203.0778            1.88m\n",
      "        77     2798030.6984            1.80m\n",
      "        78     2792848.7445            1.71m\n",
      "        79     2789302.6032            1.63m\n",
      "        80     2784223.1135            1.55m\n",
      "        81     2777604.8746            1.47m\n",
      "        82     2772419.0459            1.39m\n",
      "        83     2767878.6740            1.30m\n",
      "        84     2763354.4810            1.22m\n",
      "        85     2758475.0228            1.14m\n",
      "        86     2754041.0204            1.06m\n",
      "        87     2747180.0673           59.07s\n",
      "        88     2743185.9280           54.37s\n",
      "        89     2736463.4364           49.80s\n",
      "        90     2730573.8775           45.19s\n",
      "        91     2726796.1086           40.53s\n",
      "        92     2721672.5264           35.95s\n",
      "        93     2718518.5591           31.31s\n",
      "        94     2713143.1058           26.78s\n",
      "        95     2709553.8201           22.24s\n",
      "        96     2703637.6754           17.77s\n",
      "        97     2697074.7914           13.30s\n",
      "        98     2692664.9130            8.84s\n",
      "        99     2688791.4315            4.40s\n",
      "       100     2685102.5590            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     7594630.3597            7.06m\n",
      "         2     6985102.3409            7.09m\n",
      "         3     6484126.3346            7.01m\n",
      "         4     6068917.4514            6.98m\n",
      "         5     5716303.3696            6.95m\n",
      "         6     5424734.3296            6.90m\n",
      "         7     5168256.3320            6.86m\n",
      "         8     4941838.2866            6.84m\n",
      "         9     4754034.6748            6.82m\n",
      "        10     4578704.8942            6.74m\n",
      "        11     4431122.2638            6.68m\n",
      "        12     4304333.8954            6.59m\n",
      "        13     4187120.4332            6.52m\n",
      "        14     4093841.6041            6.42m\n",
      "        15     4003427.3400            6.31m\n",
      "        16     3920329.2386            6.21m\n",
      "        17     3849013.9424            6.14m\n",
      "        18     3781485.8879            6.06m\n",
      "        19     3719938.2385            5.97m\n",
      "        20     3663085.8052            5.88m\n",
      "        21     3608291.9416            5.82m\n",
      "        22     3563376.6570            5.75m\n",
      "        23     3520439.7423            5.68m\n",
      "        24     3475556.8387            5.62m\n",
      "        25     3445732.1595            5.54m\n",
      "        26     3411756.0010            5.45m\n",
      "        27     3383058.8748            5.34m\n",
      "        28     3355237.6316            5.24m\n",
      "        29     3330802.2412            5.15m\n",
      "        30     3299907.3717            5.10m\n",
      "        31     3274249.7079            5.02m\n",
      "        32     3252498.5003            4.93m\n",
      "        33     3226576.4619            4.87m\n",
      "        34     3209044.5581            4.79m\n",
      "        35     3188386.3789            4.71m\n",
      "        36     3169397.6707            4.63m\n",
      "        37     3155085.6199            4.53m\n",
      "        38     3138643.3372            4.45m\n",
      "        39     3124219.9227            4.36m\n",
      "        40     3107218.8828            4.28m\n",
      "        41     3092190.3286            4.21m\n",
      "        42     3076477.4061            4.14m\n",
      "        43     3065050.6577            4.06m\n",
      "        44     3049656.3089            3.99m\n",
      "        45     3037573.5546            3.92m\n",
      "        46     3025230.6991            3.84m\n",
      "        47     3013963.4830            3.75m\n",
      "        48     3001451.5531            3.68m\n",
      "        49     2991150.8219            3.60m\n",
      "        50     2979276.7286            3.52m\n",
      "        51     2966936.3416            3.46m\n",
      "        52     2957651.0064            3.37m\n",
      "        53     2946206.6043            3.30m\n",
      "        54     2935086.2665            3.23m\n",
      "        55     2927064.3009            3.15m\n",
      "        56     2918038.9208            3.08m\n",
      "        57     2909618.1906            3.00m\n",
      "        58     2898330.2770            2.94m\n",
      "        59     2891676.6931            2.87m\n",
      "        60     2879843.1418            2.82m\n",
      "        61     2871834.3037            2.74m\n",
      "        62     2864768.2999            2.67m\n",
      "        63     2856918.0264            2.59m\n",
      "        64     2848673.1947            2.52m\n",
      "        65     2836868.0021            2.45m\n",
      "        66     2830119.5561            2.37m\n",
      "        67     2822017.6940            2.30m\n",
      "        68     2813614.1407            2.22m\n",
      "        69     2805999.6502            2.14m\n",
      "        70     2800514.7865            2.07m\n",
      "        71     2791229.2446            2.01m\n",
      "        72     2787132.0774            1.93m\n",
      "        73     2779544.2424            1.86m\n",
      "        74     2773925.9871            1.79m\n",
      "        75     2767998.1726            1.72m\n",
      "        76     2761640.1574            1.65m\n",
      "        77     2752138.1215            1.58m\n",
      "        78     2743658.5260            1.51m\n",
      "        79     2737478.1835            1.44m\n",
      "        80     2729382.7182            1.37m\n",
      "        81     2723248.7788            1.30m\n",
      "        82     2718998.0764            1.23m\n",
      "        83     2713893.9547            1.16m\n",
      "        84     2709462.5544            1.09m\n",
      "        85     2702472.5518            1.02m\n",
      "        86     2697478.5117           57.28s\n",
      "        87     2691837.8713           53.17s\n",
      "        88     2686322.9003           48.98s\n",
      "        89     2681721.6908           44.79s\n",
      "        90     2676368.4470           40.69s\n",
      "        91     2672503.1831           36.51s\n",
      "        92     2665404.5217           32.46s\n",
      "        93     2661011.3222           28.35s\n",
      "        94     2658007.0876           24.22s\n",
      "        95     2651759.1213           20.18s\n",
      "        96     2646406.5878           16.15s\n",
      "        97     2643806.7648           12.08s\n",
      "        98     2638727.2585            8.05s\n",
      "        99     2635689.3666            4.01s\n",
      "       100     2631935.8183            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     7775408.5200            6.15m\n",
      "         2     7140394.0122            6.11m\n",
      "         3     6610962.6697            6.15m\n",
      "         4     6175024.0538            6.10m\n",
      "         5     5804448.7612            6.13m\n",
      "         6     5486365.7349            6.11m\n",
      "         7     5220535.1626            6.15m\n",
      "         8     4995710.7603            6.11m\n",
      "         9     4808030.2491            6.09m\n",
      "        10     4636355.9388            6.05m\n",
      "        11     4492388.1559            5.99m\n",
      "        12     4365224.0272            5.96m\n",
      "        13     4248831.7269            5.93m\n",
      "        14     4144599.2824            5.87m\n",
      "        15     4055056.1670            5.81m\n",
      "        16     3966536.1680            5.77m\n",
      "        17     3894045.7077            5.70m\n",
      "        18     3829810.2390            5.63m\n",
      "        19     3764902.0646            5.57m\n",
      "        20     3704961.5260            5.51m\n",
      "        21     3655654.6054            5.44m\n",
      "        22     3610359.3556            5.37m\n",
      "        23     3568059.8286            5.28m\n",
      "        24     3527289.6604            5.23m\n",
      "        25     3486538.7236            5.16m\n",
      "        26     3451103.2718            5.10m\n",
      "        27     3418784.3987            5.02m\n",
      "        28     3389006.8885            4.93m\n",
      "        29     3361782.9238            4.85m\n",
      "        30     3336601.2058            4.77m\n",
      "        31     3313903.1934            4.70m\n",
      "        32     3293938.0588            4.62m\n",
      "        33     3269586.8351            4.54m\n",
      "        34     3249386.0339            4.46m\n",
      "        35     3224999.2239            4.40m\n",
      "        36     3201478.6237            4.33m\n",
      "        37     3186743.6190            4.26m\n",
      "        38     3168658.9800            4.19m\n",
      "        39     3147462.2667            4.12m\n",
      "        40     3130948.2038            4.06m\n",
      "        41     3114190.7683            4.00m\n",
      "        42     3100903.4725            3.92m\n",
      "        43     3085006.7190            3.85m\n",
      "        44     3070407.3874            3.79m\n",
      "        45     3058048.3505            3.72m\n",
      "        46     3041787.0003            3.65m\n",
      "        47     3030404.1148            3.58m\n",
      "        48     3015119.7160            3.51m\n",
      "        49     3005944.3038            3.42m\n",
      "        50     2993034.8977            3.36m\n",
      "        51     2980587.8094            3.29m\n",
      "        52     2971483.3855            3.22m\n",
      "        53     2960493.7786            3.15m\n",
      "        54     2951889.2923            3.08m\n",
      "        55     2942501.7680            3.01m\n",
      "        56     2933639.5464            2.94m\n",
      "        57     2924663.1251            2.87m\n",
      "        58     2916123.6846            2.80m\n",
      "        59     2905308.1141            2.74m\n",
      "        60     2896456.3028            2.67m\n",
      "        61     2890202.6918            2.60m\n",
      "        62     2880848.4504            2.53m\n",
      "        63     2874371.2153            2.45m\n",
      "        64     2867527.7261            2.38m\n",
      "        65     2859532.1480            2.31m\n",
      "        66     2852285.7017            2.24m\n",
      "        67     2842830.7264            2.18m\n",
      "        68     2837811.9144            2.11m\n",
      "        69     2830771.8426            2.04m\n",
      "        70     2822393.9721            1.97m\n",
      "        71     2815246.1954            1.90m\n",
      "        72     2809384.3870            1.83m\n",
      "        73     2804118.8770            1.77m\n",
      "        74     2798121.6121            1.70m\n",
      "        75     2791835.2244            1.63m\n",
      "        76     2786059.1052            1.56m\n",
      "        77     2780472.1511            1.50m\n",
      "        78     2775669.6306            1.43m\n",
      "        79     2769914.9505            1.36m\n",
      "        80     2764703.8665            1.30m\n",
      "        81     2761102.6850            1.23m\n",
      "        82     2755167.7325            1.16m\n",
      "        83     2749499.4425            1.10m\n",
      "        84     2742510.8135            1.03m\n",
      "        85     2738061.8033           58.06s\n",
      "        86     2733225.4612           54.11s\n",
      "        87     2729534.6657           50.13s\n",
      "        88     2723090.6610           46.27s\n",
      "        89     2719265.1354           42.39s\n",
      "        90     2712473.4186           38.55s\n",
      "        91     2707107.2881           34.65s\n",
      "        92     2700502.0175           30.80s\n",
      "        93     2696505.1783           26.93s\n",
      "        94     2692747.8944           23.03s\n",
      "        95     2689138.8990           19.17s\n",
      "        96     2685047.9571           15.31s\n",
      "        97     2680617.5345           11.46s\n",
      "        98     2673181.5433            7.64s\n",
      "        99     2669592.5307            3.81s\n",
      "       100     2666238.2316            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     7681823.3023           10.96m\n",
      "         2     7067982.5146           10.88m\n",
      "         3     6553409.3948           10.95m\n",
      "         4     6124716.1501           10.98m\n",
      "         5     5771097.3276           10.82m\n",
      "         6     5463771.0072           10.78m\n",
      "         7     5213473.5017           10.69m\n",
      "         8     4996422.0649           10.60m\n",
      "         9     4807716.0211           10.48m\n",
      "        10     4648530.6489           10.39m\n",
      "        11     4498666.0534           10.32m\n",
      "        12     4375148.2731           10.16m\n",
      "        13     4266466.7851           10.08m\n",
      "        14     4167726.9886            9.98m\n",
      "        15     4082777.3341            9.86m\n",
      "        16     4004128.8725            9.75m\n",
      "        17     3937172.4901            9.63m\n",
      "        18     3875957.7949            9.49m\n",
      "        19     3815082.4953            9.39m\n",
      "        20     3767506.3913            9.26m\n",
      "        21     3718673.4044            9.12m\n",
      "        22     3672866.8402            9.04m\n",
      "        23     3631979.4143            8.90m\n",
      "        24     3595811.4513            8.75m\n",
      "        25     3561591.9450            8.63m\n",
      "        26     3531783.6560            8.49m\n",
      "        27     3496691.3241            8.38m\n",
      "        28     3469927.9529            8.25m\n",
      "        29     3443564.4547            8.12m\n",
      "        30     3418524.0802            8.01m\n",
      "        31     3389988.3897            7.91m\n",
      "        32     3369506.5221            7.77m\n",
      "        33     3347345.7965            7.63m\n",
      "        34     3331431.2196            7.50m\n",
      "        35     3312842.0242            7.38m\n",
      "        36     3291932.6687            7.26m\n",
      "        37     3275124.5450            7.13m\n",
      "        38     3259581.8428            7.03m\n",
      "        39     3244818.3930            6.90m\n",
      "        40     3228275.4723            6.78m\n",
      "        41     3217230.6199            6.65m\n",
      "        42     3204986.2132            6.53m\n",
      "        43     3192095.3832            6.40m\n",
      "        44     3178409.9786            6.30m\n",
      "        45     3163600.3374            6.18m\n",
      "        46     3151656.8894            6.07m\n",
      "        47     3138455.4182            5.95m\n",
      "        48     3127524.6220            5.84m\n",
      "        49     3115362.0087            5.73m\n",
      "        50     3104841.3016            5.61m\n",
      "        51     3095548.1310            5.48m\n",
      "        52     3085071.9391            5.36m\n",
      "        53     3078025.1122            5.23m\n",
      "        54     3069978.4480            5.11m\n",
      "        55     3061736.2215            5.00m\n",
      "        56     3055099.0759            4.87m\n",
      "        57     3045536.1078            4.76m\n",
      "        58     3035719.2698            4.65m\n",
      "        59     3027620.5086            4.53m\n",
      "        60     3021656.9844            4.42m\n",
      "        61     3013595.4158            4.30m\n",
      "        62     3005884.8190            4.19m\n",
      "        63     2999620.1290            4.07m\n",
      "        64     2991980.0456            3.96m\n",
      "        65     2980646.3977            3.86m\n",
      "        66     2976615.5197            3.73m\n",
      "        67     2969260.4788            3.62m\n",
      "        68     2960438.0699            3.52m\n",
      "        69     2955140.5276            3.40m\n",
      "        70     2947488.2411            3.29m\n",
      "        71     2941690.3505            3.17m\n",
      "        72     2935567.1470            3.06m\n",
      "        73     2929660.4461            2.96m\n",
      "        74     2922764.8937            2.86m\n",
      "        75     2916386.3463            2.76m\n",
      "        76     2906939.3177            2.67m\n",
      "        77     2902792.3584            2.55m\n",
      "        78     2897844.0265            2.45m\n",
      "        79     2894112.4356            2.33m\n",
      "        80     2888552.7720            2.22m\n",
      "        81     2884166.3124            2.11m\n",
      "        82     2878514.5200            2.00m\n",
      "        83     2873442.8639            1.89m\n",
      "        84     2869469.3705            1.78m\n",
      "        85     2865445.9434            1.67m\n",
      "        86     2859825.4070            1.56m\n",
      "        87     2854769.1769            1.45m\n",
      "        88     2851710.8463            1.33m\n",
      "        89     2848134.4071            1.22m\n",
      "        90     2842726.6204            1.11m\n",
      "        91     2837155.0335            1.00m\n",
      "        92     2832387.7342           53.47s\n",
      "        93     2827450.1009           46.83s\n",
      "        94     2823801.7127           40.13s\n",
      "        95     2819757.2616           33.40s\n",
      "        96     2816381.6109           26.69s\n",
      "        97     2811178.8122           20.03s\n",
      "        98     2807369.6846           13.34s\n",
      "        99     2804250.7678            6.66s\n",
      "       100     2800926.8538            0.00s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters={'learning_rate':[1e-1],'min_samples_split':[7],'verbose':[2],'max_depth':[7],'min_samples_leaf':[1],'subsample':[1.0],'loss':['ls'],'n_estimators':[100]}\n",
    "clf = grid_search.GridSearchCV(GradientBoostingRegressor(),parameters) \n",
    "clf.fit(data_train[predictors],data_train['loss'])\n",
    "data_predictions = clf.predict(data_test[predictors])\n",
    "\n",
    "data_test['loss'] = data_predictions\n",
    "outputFrame = pandas.DataFrame()\n",
    "outputFrame['id'] = data_test['id']\n",
    "outputFrame['loss'] = data_test['loss']\n",
    "outputFrame.to_csv(\"outputgbm.csv\",index =False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
