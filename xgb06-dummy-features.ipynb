{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.cross_validation  import KFold\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import os\n",
    "os.chdir(\"/home/udit/ipython/notebook/all/input\")\n",
    "\n",
    "shift = 200\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')\n",
    "            \n",
    "\n",
    "def encode(charcode):\n",
    "    r = 0\n",
    "    ln = len(str(charcode))\n",
    "    for i in range(ln):\n",
    "        r += (ord(str(charcode)[i]) - ord('A') + 1) * 26 ** (ln - i - 1)\n",
    "    return r\n",
    "\n",
    "fair_constant = 0.7\n",
    "def fair_obj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    x = (preds - labels)\n",
    "    den = abs(x) + fair_constant\n",
    "    grad = fair_constant * x / (den)\n",
    "    hess = fair_constant * fair_constant / (den * den)\n",
    "    return grad, hess\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-shift,\n",
    "                                      np.exp(yhat)-shift)\n",
    "def mungeskewed(train, test, numeric_feats):\n",
    "    ntrain = train.shape[0]\n",
    "    test['loss'] = 0\n",
    "    train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.25]\n",
    "    skewed_feats = skewed_feats.index\n",
    "\n",
    "    for feats in skewed_feats:\n",
    "        train_test[feats] = train_test[feats] + 1\n",
    "        train_test[feats], lam = boxcox(train_test[feats])\n",
    "    return train_test, ntrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Started\n",
      "\n",
      "('Combining Columns:', 'cat80_cat87')\n",
      "('Combining Columns:', 'cat80_cat57')\n",
      "('Combining Columns:', 'cat80_cat12')\n",
      "('Combining Columns:', 'cat80_cat79')\n",
      "('Combining Columns:', 'cat80_cat10')\n",
      "('Combining Columns:', 'cat80_cat7')\n",
      "('Combining Columns:', 'cat80_cat89')\n",
      "('Combining Columns:', 'cat80_cat2')\n",
      "('Combining Columns:', 'cat80_cat72')\n",
      "('Combining Columns:', 'cat80_cat81')\n",
      "('Combining Columns:', 'cat80_cat11')\n",
      "('Combining Columns:', 'cat80_cat1')\n",
      "('Combining Columns:', 'cat80_cat13')\n",
      "('Combining Columns:', 'cat80_cat9')\n",
      "('Combining Columns:', 'cat80_cat3')\n",
      "('Combining Columns:', 'cat80_cat16')\n",
      "('Combining Columns:', 'cat80_cat90')\n",
      "('Combining Columns:', 'cat80_cat23')\n",
      "('Combining Columns:', 'cat80_cat36')\n",
      "('Combining Columns:', 'cat80_cat73')\n",
      "('Combining Columns:', 'cat80_cat103')\n",
      "('Combining Columns:', 'cat80_cat40')\n",
      "('Combining Columns:', 'cat80_cat28')\n",
      "('Combining Columns:', 'cat80_cat111')\n",
      "('Combining Columns:', 'cat80_cat6')\n",
      "('Combining Columns:', 'cat80_cat76')\n",
      "('Combining Columns:', 'cat80_cat50')\n",
      "('Combining Columns:', 'cat80_cat5')\n",
      "('Combining Columns:', 'cat80_cat4')\n",
      "('Combining Columns:', 'cat80_cat14')\n",
      "('Combining Columns:', 'cat80_cat38')\n",
      "('Combining Columns:', 'cat80_cat24')\n",
      "('Combining Columns:', 'cat80_cat82')\n",
      "('Combining Columns:', 'cat80_cat25')\n",
      "('Combining Columns:', 'cat87_cat57')\n",
      "('Combining Columns:', 'cat87_cat12')\n",
      "('Combining Columns:', 'cat87_cat79')\n",
      "('Combining Columns:', 'cat87_cat10')\n",
      "('Combining Columns:', 'cat87_cat7')\n",
      "('Combining Columns:', 'cat87_cat89')\n",
      "('Combining Columns:', 'cat87_cat2')\n",
      "('Combining Columns:', 'cat87_cat72')\n",
      "('Combining Columns:', 'cat87_cat81')\n",
      "('Combining Columns:', 'cat87_cat11')\n",
      "('Combining Columns:', 'cat87_cat1')\n",
      "('Combining Columns:', 'cat87_cat13')\n",
      "('Combining Columns:', 'cat87_cat9')\n",
      "('Combining Columns:', 'cat87_cat3')\n",
      "('Combining Columns:', 'cat87_cat16')\n",
      "('Combining Columns:', 'cat87_cat90')\n",
      "('Combining Columns:', 'cat87_cat23')\n",
      "('Combining Columns:', 'cat87_cat36')\n",
      "('Combining Columns:', 'cat87_cat73')\n",
      "('Combining Columns:', 'cat87_cat103')\n",
      "('Combining Columns:', 'cat87_cat40')\n",
      "('Combining Columns:', 'cat87_cat28')\n",
      "('Combining Columns:', 'cat87_cat111')\n",
      "('Combining Columns:', 'cat87_cat6')\n",
      "('Combining Columns:', 'cat87_cat76')\n",
      "('Combining Columns:', 'cat87_cat50')\n",
      "('Combining Columns:', 'cat87_cat5')\n",
      "('Combining Columns:', 'cat87_cat4')\n",
      "('Combining Columns:', 'cat87_cat14')\n",
      "('Combining Columns:', 'cat87_cat38')\n",
      "('Combining Columns:', 'cat87_cat24')\n",
      "('Combining Columns:', 'cat87_cat82')\n",
      "('Combining Columns:', 'cat87_cat25')\n",
      "('Combining Columns:', 'cat57_cat12')\n",
      "('Combining Columns:', 'cat57_cat79')\n",
      "('Combining Columns:', 'cat57_cat10')\n",
      "('Combining Columns:', 'cat57_cat7')\n",
      "('Combining Columns:', 'cat57_cat89')\n",
      "('Combining Columns:', 'cat57_cat2')\n",
      "('Combining Columns:', 'cat57_cat72')\n",
      "('Combining Columns:', 'cat57_cat81')\n",
      "('Combining Columns:', 'cat57_cat11')\n",
      "('Combining Columns:', 'cat57_cat1')\n",
      "('Combining Columns:', 'cat57_cat13')\n",
      "('Combining Columns:', 'cat57_cat9')\n",
      "('Combining Columns:', 'cat57_cat3')\n",
      "('Combining Columns:', 'cat57_cat16')\n",
      "('Combining Columns:', 'cat57_cat90')\n",
      "('Combining Columns:', 'cat57_cat23')\n",
      "('Combining Columns:', 'cat57_cat36')\n",
      "('Combining Columns:', 'cat57_cat73')\n",
      "('Combining Columns:', 'cat57_cat103')\n",
      "('Combining Columns:', 'cat57_cat40')\n",
      "('Combining Columns:', 'cat57_cat28')\n",
      "('Combining Columns:', 'cat57_cat111')\n",
      "('Combining Columns:', 'cat57_cat6')\n",
      "('Combining Columns:', 'cat57_cat76')\n",
      "('Combining Columns:', 'cat57_cat50')\n",
      "('Combining Columns:', 'cat57_cat5')\n",
      "('Combining Columns:', 'cat57_cat4')\n",
      "('Combining Columns:', 'cat57_cat14')\n",
      "('Combining Columns:', 'cat57_cat38')\n",
      "('Combining Columns:', 'cat57_cat24')\n",
      "('Combining Columns:', 'cat57_cat82')\n",
      "('Combining Columns:', 'cat57_cat25')\n",
      "('Combining Columns:', 'cat12_cat79')\n",
      "('Combining Columns:', 'cat12_cat10')\n",
      "('Combining Columns:', 'cat12_cat7')\n",
      "('Combining Columns:', 'cat12_cat89')\n",
      "('Combining Columns:', 'cat12_cat2')\n",
      "('Combining Columns:', 'cat12_cat72')\n",
      "('Combining Columns:', 'cat12_cat81')\n",
      "('Combining Columns:', 'cat12_cat11')\n",
      "('Combining Columns:', 'cat12_cat1')\n",
      "('Combining Columns:', 'cat12_cat13')\n",
      "('Combining Columns:', 'cat12_cat9')\n",
      "('Combining Columns:', 'cat12_cat3')\n",
      "('Combining Columns:', 'cat12_cat16')\n",
      "('Combining Columns:', 'cat12_cat90')\n",
      "('Combining Columns:', 'cat12_cat23')\n",
      "('Combining Columns:', 'cat12_cat36')\n",
      "('Combining Columns:', 'cat12_cat73')\n",
      "('Combining Columns:', 'cat12_cat103')\n",
      "('Combining Columns:', 'cat12_cat40')\n",
      "('Combining Columns:', 'cat12_cat28')\n",
      "('Combining Columns:', 'cat12_cat111')\n",
      "('Combining Columns:', 'cat12_cat6')\n",
      "('Combining Columns:', 'cat12_cat76')\n",
      "('Combining Columns:', 'cat12_cat50')\n",
      "('Combining Columns:', 'cat12_cat5')\n",
      "('Combining Columns:', 'cat12_cat4')\n",
      "('Combining Columns:', 'cat12_cat14')\n",
      "('Combining Columns:', 'cat12_cat38')\n",
      "('Combining Columns:', 'cat12_cat24')\n",
      "('Combining Columns:', 'cat12_cat82')\n",
      "('Combining Columns:', 'cat12_cat25')\n",
      "('Combining Columns:', 'cat79_cat10')\n",
      "('Combining Columns:', 'cat79_cat7')\n",
      "('Combining Columns:', 'cat79_cat89')\n",
      "('Combining Columns:', 'cat79_cat2')\n",
      "('Combining Columns:', 'cat79_cat72')\n",
      "('Combining Columns:', 'cat79_cat81')\n",
      "('Combining Columns:', 'cat79_cat11')\n",
      "('Combining Columns:', 'cat79_cat1')\n",
      "('Combining Columns:', 'cat79_cat13')\n",
      "('Combining Columns:', 'cat79_cat9')\n",
      "('Combining Columns:', 'cat79_cat3')\n",
      "('Combining Columns:', 'cat79_cat16')\n",
      "('Combining Columns:', 'cat79_cat90')\n",
      "('Combining Columns:', 'cat79_cat23')\n",
      "('Combining Columns:', 'cat79_cat36')\n",
      "('Combining Columns:', 'cat79_cat73')\n",
      "('Combining Columns:', 'cat79_cat103')\n",
      "('Combining Columns:', 'cat79_cat40')\n",
      "('Combining Columns:', 'cat79_cat28')\n",
      "('Combining Columns:', 'cat79_cat111')\n",
      "('Combining Columns:', 'cat79_cat6')\n",
      "('Combining Columns:', 'cat79_cat76')\n",
      "('Combining Columns:', 'cat79_cat50')\n",
      "('Combining Columns:', 'cat79_cat5')\n",
      "('Combining Columns:', 'cat79_cat4')\n",
      "('Combining Columns:', 'cat79_cat14')\n",
      "('Combining Columns:', 'cat79_cat38')\n",
      "('Combining Columns:', 'cat79_cat24')\n",
      "('Combining Columns:', 'cat79_cat82')\n",
      "('Combining Columns:', 'cat79_cat25')\n",
      "('Combining Columns:', 'cat10_cat7')\n",
      "('Combining Columns:', 'cat10_cat89')\n",
      "('Combining Columns:', 'cat10_cat2')\n",
      "('Combining Columns:', 'cat10_cat72')\n",
      "('Combining Columns:', 'cat10_cat81')\n",
      "('Combining Columns:', 'cat10_cat11')\n",
      "('Combining Columns:', 'cat10_cat1')\n",
      "('Combining Columns:', 'cat10_cat13')\n",
      "('Combining Columns:', 'cat10_cat9')\n",
      "('Combining Columns:', 'cat10_cat3')\n",
      "('Combining Columns:', 'cat10_cat16')\n",
      "('Combining Columns:', 'cat10_cat90')\n",
      "('Combining Columns:', 'cat10_cat23')\n",
      "('Combining Columns:', 'cat10_cat36')\n",
      "('Combining Columns:', 'cat10_cat73')\n",
      "('Combining Columns:', 'cat10_cat103')\n",
      "('Combining Columns:', 'cat10_cat40')\n",
      "('Combining Columns:', 'cat10_cat28')\n",
      "('Combining Columns:', 'cat10_cat111')\n",
      "('Combining Columns:', 'cat10_cat6')\n",
      "('Combining Columns:', 'cat10_cat76')\n",
      "('Combining Columns:', 'cat10_cat50')\n",
      "('Combining Columns:', 'cat10_cat5')\n",
      "('Combining Columns:', 'cat10_cat4')\n",
      "('Combining Columns:', 'cat10_cat14')\n",
      "('Combining Columns:', 'cat10_cat38')\n",
      "('Combining Columns:', 'cat10_cat24')\n",
      "('Combining Columns:', 'cat10_cat82')\n",
      "('Combining Columns:', 'cat10_cat25')\n",
      "('Combining Columns:', 'cat7_cat89')\n",
      "('Combining Columns:', 'cat7_cat2')\n",
      "('Combining Columns:', 'cat7_cat72')\n",
      "('Combining Columns:', 'cat7_cat81')\n",
      "('Combining Columns:', 'cat7_cat11')\n",
      "('Combining Columns:', 'cat7_cat1')\n",
      "('Combining Columns:', 'cat7_cat13')\n",
      "('Combining Columns:', 'cat7_cat9')\n",
      "('Combining Columns:', 'cat7_cat3')\n",
      "('Combining Columns:', 'cat7_cat16')\n",
      "('Combining Columns:', 'cat7_cat90')\n",
      "('Combining Columns:', 'cat7_cat23')\n",
      "('Combining Columns:', 'cat7_cat36')\n",
      "('Combining Columns:', 'cat7_cat73')\n",
      "('Combining Columns:', 'cat7_cat103')\n",
      "('Combining Columns:', 'cat7_cat40')\n",
      "('Combining Columns:', 'cat7_cat28')\n",
      "('Combining Columns:', 'cat7_cat111')\n",
      "('Combining Columns:', 'cat7_cat6')\n",
      "('Combining Columns:', 'cat7_cat76')\n",
      "('Combining Columns:', 'cat7_cat50')\n",
      "('Combining Columns:', 'cat7_cat5')\n",
      "('Combining Columns:', 'cat7_cat4')\n",
      "('Combining Columns:', 'cat7_cat14')\n",
      "('Combining Columns:', 'cat7_cat38')\n",
      "('Combining Columns:', 'cat7_cat24')\n",
      "('Combining Columns:', 'cat7_cat82')\n",
      "('Combining Columns:', 'cat7_cat25')\n",
      "('Combining Columns:', 'cat89_cat2')\n",
      "('Combining Columns:', 'cat89_cat72')\n",
      "('Combining Columns:', 'cat89_cat81')\n",
      "('Combining Columns:', 'cat89_cat11')\n",
      "('Combining Columns:', 'cat89_cat1')\n",
      "('Combining Columns:', 'cat89_cat13')\n",
      "('Combining Columns:', 'cat89_cat9')\n",
      "('Combining Columns:', 'cat89_cat3')\n",
      "('Combining Columns:', 'cat89_cat16')\n",
      "('Combining Columns:', 'cat89_cat90')\n",
      "('Combining Columns:', 'cat89_cat23')\n",
      "('Combining Columns:', 'cat89_cat36')\n",
      "('Combining Columns:', 'cat89_cat73')\n",
      "('Combining Columns:', 'cat89_cat103')\n",
      "('Combining Columns:', 'cat89_cat40')\n",
      "('Combining Columns:', 'cat89_cat28')\n",
      "('Combining Columns:', 'cat89_cat111')\n",
      "('Combining Columns:', 'cat89_cat6')\n",
      "('Combining Columns:', 'cat89_cat76')\n",
      "('Combining Columns:', 'cat89_cat50')\n",
      "('Combining Columns:', 'cat89_cat5')\n",
      "('Combining Columns:', 'cat89_cat4')\n",
      "('Combining Columns:', 'cat89_cat14')\n",
      "('Combining Columns:', 'cat89_cat38')\n",
      "('Combining Columns:', 'cat89_cat24')\n",
      "('Combining Columns:', 'cat89_cat82')\n",
      "('Combining Columns:', 'cat89_cat25')\n",
      "('Combining Columns:', 'cat2_cat72')\n",
      "('Combining Columns:', 'cat2_cat81')\n",
      "('Combining Columns:', 'cat2_cat11')\n",
      "('Combining Columns:', 'cat2_cat1')\n",
      "('Combining Columns:', 'cat2_cat13')\n",
      "('Combining Columns:', 'cat2_cat9')\n",
      "('Combining Columns:', 'cat2_cat3')\n",
      "('Combining Columns:', 'cat2_cat16')\n",
      "('Combining Columns:', 'cat2_cat90')\n",
      "('Combining Columns:', 'cat2_cat23')\n",
      "('Combining Columns:', 'cat2_cat36')\n",
      "('Combining Columns:', 'cat2_cat73')\n",
      "('Combining Columns:', 'cat2_cat103')\n",
      "('Combining Columns:', 'cat2_cat40')\n",
      "('Combining Columns:', 'cat2_cat28')\n",
      "('Combining Columns:', 'cat2_cat111')\n",
      "('Combining Columns:', 'cat2_cat6')\n",
      "('Combining Columns:', 'cat2_cat76')\n",
      "('Combining Columns:', 'cat2_cat50')\n",
      "('Combining Columns:', 'cat2_cat5')\n",
      "('Combining Columns:', 'cat2_cat4')\n",
      "('Combining Columns:', 'cat2_cat14')\n",
      "('Combining Columns:', 'cat2_cat38')\n",
      "('Combining Columns:', 'cat2_cat24')\n",
      "('Combining Columns:', 'cat2_cat82')\n",
      "('Combining Columns:', 'cat2_cat25')\n",
      "('Combining Columns:', 'cat72_cat81')\n",
      "('Combining Columns:', 'cat72_cat11')\n",
      "('Combining Columns:', 'cat72_cat1')\n",
      "('Combining Columns:', 'cat72_cat13')\n",
      "('Combining Columns:', 'cat72_cat9')\n",
      "('Combining Columns:', 'cat72_cat3')\n",
      "('Combining Columns:', 'cat72_cat16')\n",
      "('Combining Columns:', 'cat72_cat90')\n",
      "('Combining Columns:', 'cat72_cat23')\n",
      "('Combining Columns:', 'cat72_cat36')\n",
      "('Combining Columns:', 'cat72_cat73')\n",
      "('Combining Columns:', 'cat72_cat103')\n",
      "('Combining Columns:', 'cat72_cat40')\n",
      "('Combining Columns:', 'cat72_cat28')\n",
      "('Combining Columns:', 'cat72_cat111')\n",
      "('Combining Columns:', 'cat72_cat6')\n",
      "('Combining Columns:', 'cat72_cat76')\n",
      "('Combining Columns:', 'cat72_cat50')\n",
      "('Combining Columns:', 'cat72_cat5')\n",
      "('Combining Columns:', 'cat72_cat4')\n",
      "('Combining Columns:', 'cat72_cat14')\n",
      "('Combining Columns:', 'cat72_cat38')\n",
      "('Combining Columns:', 'cat72_cat24')\n",
      "('Combining Columns:', 'cat72_cat82')\n",
      "('Combining Columns:', 'cat72_cat25')\n",
      "('Combining Columns:', 'cat81_cat11')\n",
      "('Combining Columns:', 'cat81_cat1')\n",
      "('Combining Columns:', 'cat81_cat13')\n",
      "('Combining Columns:', 'cat81_cat9')\n",
      "('Combining Columns:', 'cat81_cat3')\n",
      "('Combining Columns:', 'cat81_cat16')\n",
      "('Combining Columns:', 'cat81_cat90')\n",
      "('Combining Columns:', 'cat81_cat23')\n",
      "('Combining Columns:', 'cat81_cat36')\n",
      "('Combining Columns:', 'cat81_cat73')\n",
      "('Combining Columns:', 'cat81_cat103')\n",
      "('Combining Columns:', 'cat81_cat40')\n",
      "('Combining Columns:', 'cat81_cat28')\n",
      "('Combining Columns:', 'cat81_cat111')\n",
      "('Combining Columns:', 'cat81_cat6')\n",
      "('Combining Columns:', 'cat81_cat76')\n",
      "('Combining Columns:', 'cat81_cat50')\n",
      "('Combining Columns:', 'cat81_cat5')\n",
      "('Combining Columns:', 'cat81_cat4')\n",
      "('Combining Columns:', 'cat81_cat14')\n",
      "('Combining Columns:', 'cat81_cat38')\n",
      "('Combining Columns:', 'cat81_cat24')\n",
      "('Combining Columns:', 'cat81_cat82')\n",
      "('Combining Columns:', 'cat81_cat25')\n",
      "('Combining Columns:', 'cat11_cat1')\n",
      "('Combining Columns:', 'cat11_cat13')\n",
      "('Combining Columns:', 'cat11_cat9')\n",
      "('Combining Columns:', 'cat11_cat3')\n",
      "('Combining Columns:', 'cat11_cat16')\n",
      "('Combining Columns:', 'cat11_cat90')\n",
      "('Combining Columns:', 'cat11_cat23')\n",
      "('Combining Columns:', 'cat11_cat36')\n",
      "('Combining Columns:', 'cat11_cat73')\n",
      "('Combining Columns:', 'cat11_cat103')\n",
      "('Combining Columns:', 'cat11_cat40')\n",
      "('Combining Columns:', 'cat11_cat28')\n",
      "('Combining Columns:', 'cat11_cat111')\n",
      "('Combining Columns:', 'cat11_cat6')\n",
      "('Combining Columns:', 'cat11_cat76')\n",
      "('Combining Columns:', 'cat11_cat50')\n",
      "('Combining Columns:', 'cat11_cat5')\n",
      "('Combining Columns:', 'cat11_cat4')\n",
      "('Combining Columns:', 'cat11_cat14')\n",
      "('Combining Columns:', 'cat11_cat38')\n",
      "('Combining Columns:', 'cat11_cat24')\n",
      "('Combining Columns:', 'cat11_cat82')\n",
      "('Combining Columns:', 'cat11_cat25')\n",
      "('Combining Columns:', 'cat1_cat13')\n",
      "('Combining Columns:', 'cat1_cat9')\n",
      "('Combining Columns:', 'cat1_cat3')\n",
      "('Combining Columns:', 'cat1_cat16')\n",
      "('Combining Columns:', 'cat1_cat90')\n",
      "('Combining Columns:', 'cat1_cat23')\n",
      "('Combining Columns:', 'cat1_cat36')\n",
      "('Combining Columns:', 'cat1_cat73')\n",
      "('Combining Columns:', 'cat1_cat103')\n",
      "('Combining Columns:', 'cat1_cat40')\n",
      "('Combining Columns:', 'cat1_cat28')\n",
      "('Combining Columns:', 'cat1_cat111')\n",
      "('Combining Columns:', 'cat1_cat6')\n",
      "('Combining Columns:', 'cat1_cat76')\n",
      "('Combining Columns:', 'cat1_cat50')\n",
      "('Combining Columns:', 'cat1_cat5')\n",
      "('Combining Columns:', 'cat1_cat4')\n",
      "('Combining Columns:', 'cat1_cat14')\n",
      "('Combining Columns:', 'cat1_cat38')\n",
      "('Combining Columns:', 'cat1_cat24')\n",
      "('Combining Columns:', 'cat1_cat82')\n",
      "('Combining Columns:', 'cat1_cat25')\n",
      "('Combining Columns:', 'cat13_cat9')\n",
      "('Combining Columns:', 'cat13_cat3')\n",
      "('Combining Columns:', 'cat13_cat16')\n",
      "('Combining Columns:', 'cat13_cat90')\n",
      "('Combining Columns:', 'cat13_cat23')\n",
      "('Combining Columns:', 'cat13_cat36')\n",
      "('Combining Columns:', 'cat13_cat73')\n",
      "('Combining Columns:', 'cat13_cat103')\n",
      "('Combining Columns:', 'cat13_cat40')\n",
      "('Combining Columns:', 'cat13_cat28')\n",
      "('Combining Columns:', 'cat13_cat111')\n",
      "('Combining Columns:', 'cat13_cat6')\n",
      "('Combining Columns:', 'cat13_cat76')\n",
      "('Combining Columns:', 'cat13_cat50')\n",
      "('Combining Columns:', 'cat13_cat5')\n",
      "('Combining Columns:', 'cat13_cat4')\n",
      "('Combining Columns:', 'cat13_cat14')\n",
      "('Combining Columns:', 'cat13_cat38')\n",
      "('Combining Columns:', 'cat13_cat24')\n",
      "('Combining Columns:', 'cat13_cat82')\n",
      "('Combining Columns:', 'cat13_cat25')\n",
      "('Combining Columns:', 'cat9_cat3')\n",
      "('Combining Columns:', 'cat9_cat16')\n",
      "('Combining Columns:', 'cat9_cat90')\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-728b56300f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMB_FEATURE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtrain_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mtrain_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Combining Columns:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2423\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2424\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   3416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3417\u001b[0m             \u001b[0;31m# This item wasn't present, just insert at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3418\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3419\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   3544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3548\u001b[0m     def reindex_axis(self, new_index, axis, method=None, limit=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   4267\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4268\u001b[0m         merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\n\u001b[0;32m-> 4269\u001b[0;31m                                       _can_consolidate=_can_consolidate)\n\u001b[0m\u001b[1;32m   4270\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   4290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4291\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4292\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4293\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('\\nStarted')\n",
    "directory = '../input/'\n",
    "train = pd.read_csv(directory + 'train.csv')\n",
    "test = pd.read_csv(directory + 'test.csv')\n",
    "\n",
    "numeric_feats = [x for x in train.columns[1:-1] if 'cont' in x]\n",
    "categorical_feats = [x for x in train.columns[1:-1] if 'cat' in x]\n",
    "train_test, ntrain = mungeskewed(train, test, numeric_feats)\n",
    "\n",
    "# taken from Vladimir's script (https://www.kaggle.com/iglovikov/allstate-claims-severity/xgb-1114)\n",
    "for column in list(train.select_dtypes(include=['object']).columns):\n",
    "    if train[column].nunique() != test[column].nunique():\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        remove_train = set_train - set_test\n",
    "        remove_test = set_test - set_train\n",
    "\n",
    "        remove = remove_train.union(remove_test)\n",
    "\n",
    "\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "\n",
    "\n",
    "        train_test[column] = train_test[column].apply(lambda x: filter_cat(x), 1)\n",
    "\n",
    "# taken from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "train_test[\"cont1\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont1\"]))\n",
    "train_test[\"cont4\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont4\"]))\n",
    "train_test[\"cont5\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont5\"]))\n",
    "train_test[\"cont8\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont8\"]))\n",
    "train_test[\"cont10\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont10\"]))\n",
    "train_test[\"cont11\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont11\"]))\n",
    "train_test[\"cont12\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont12\"]))\n",
    "\n",
    "train_test[\"cont6\"] = np.log(preprocessing.minmax_scale(train_test[\"cont6\"]) + 0000.1)\n",
    "train_test[\"cont7\"] = np.log(preprocessing.minmax_scale(train_test[\"cont7\"]) + 0000.1)\n",
    "train_test[\"cont9\"] = np.log(preprocessing.minmax_scale(train_test[\"cont9\"]) + 0000.1)\n",
    "train_test[\"cont13\"] = np.log(preprocessing.minmax_scale(train_test[\"cont13\"]) + 0000.1)\n",
    "train_test[\"cont14\"] = (np.maximum(train_test[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n",
    "\n",
    "print('')\n",
    "for comb in itertools.combinations(COMB_FEATURE, 2):\n",
    "    feat = comb[0] + \"_\" + comb[1]\n",
    "    train_test[feat] = train_test[comb[0]] + train_test[comb[1]]\n",
    "    train_test[feat] = train_test[feat].apply(encode)\n",
    "    print('Combining Columns:', feat)\n",
    "\n",
    "print('')\n",
    "for col in categorical_feats:\n",
    "    print('Analyzing Column:', col)\n",
    "    train_test[col] = train_test[col].apply(encode)\n",
    "\n",
    "print(train_test[categorical_feats])\n",
    "\n",
    "ss = StandardScaler()\n",
    "train_test[numeric_feats] = \\\n",
    "    ss.fit_transform(train_test[numeric_feats].values)\n",
    "\n",
    "train = train_test.iloc[:ntrain, :].copy()\n",
    "test = train_test.iloc[ntrain:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('processed_trained.csv')\n",
    "test=pd.read_csv('processed_test.csv')\n",
    "#train.to_csv('processed_trained.csv')\n",
    "#test.to_csv('processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train.astype(float)\n",
    "test=test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMedian Loss:', 2115.5699999999997)\n",
      "('Mean Loss:', 3037.3376856699833)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMedian Loss:', train.loss.median())\n",
    "print('Mean Loss:', train.loss.mean())\n",
    "\n",
    "##Add dummy features\n",
    "var=['cat101','cat87','cat10']\n",
    "dummay=pd.get_dummies(train[var].astype(str))\n",
    "train=pd.concat([train,dummay],axis=1)\n",
    "dummay=pd.get_dummies(test[var].astype(str))\n",
    "test=pd.concat([test,dummay],axis=1)\n",
    "train=train.drop(var,axis=1)\n",
    "test=test.drop(var,axis=1)\n",
    "#desvar=['cat79', 'cat101', 'cat87', 'cat57', 'cat12', 'cat10', 'cat7',\n",
    "#      'cat89', 'cat2', 'cat72', 'cat11', 'cat13']\n",
    "#for col in desvar:\n",
    "#    mapmean=train.groupby(col).loss.mean()\n",
    "#    train.loc[:,'mean_'+col]=train[col].map(dict(mapmean/max(mapmean)))\n",
    "#    test.loc[:,'mean_'+col]=test[col].map(dict(mapmean/max(mapmean)))\n",
    "#    stdmean=train.groupby(col).loss.std()\n",
    "#    train.loc[:,'std_'+col]=train[col].map(dict(stdmean/max(mapmean)))\n",
    "#    test.loc[:,'std_'+col]=test[col].map(dict(stdmean/max(mapmean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=train.drop('cat101_32000.0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=train.fillna(-1)\n",
    "test=test.fillna(-1)\n",
    "ids = pd.read_csv('../input/test.csv')['id']\n",
    "train_y = np.log(train['loss'] + shift)\n",
    "train_x = train.drop(['loss','id'], axis=1)\n",
    "test_x = test.drop(['loss','id'], axis=1)\n",
    "\n",
    "n_folds = 10\n",
    "cv_sum = 0\n",
    "early_stopping = 100\n",
    "fpred = []\n",
    "xgb_rounds = []\n",
    "\n",
    "d_train_full = xgb.DMatrix(train_x, label=train_y)\n",
    "d_test = xgb.DMatrix(test_x)\n",
    "\n",
    "kf = KFold(train.shape[0], n_folds=n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1\n",
      "[0]\ttrain-mae:3234.02\teval-mae:3242.34\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2779.31\teval-mae:2786.4\n",
      "[100]\ttrain-mae:2230.42\teval-mae:2235.91\n",
      "[150]\ttrain-mae:1857.16\teval-mae:1861.99\n",
      "[200]\ttrain-mae:1610.99\teval-mae:1617.32\n",
      "[250]\ttrain-mae:1448.29\teval-mae:1457.75\n",
      "[300]\ttrain-mae:1340.66\teval-mae:1353.83\n",
      "[350]\ttrain-mae:1269\teval-mae:1285.77\n",
      "[400]\ttrain-mae:1221.62\teval-mae:1241.56\n",
      "[450]\ttrain-mae:1189.85\teval-mae:1212.92\n",
      "[500]\ttrain-mae:1168.17\teval-mae:1194.05\n",
      "[550]\ttrain-mae:1153.13\teval-mae:1181.43\n",
      "[600]\ttrain-mae:1142.45\teval-mae:1172.88\n",
      "[650]\ttrain-mae:1134.3\teval-mae:1166.85\n",
      "[700]\ttrain-mae:1128.19\teval-mae:1162.65\n",
      "[750]\ttrain-mae:1123.37\teval-mae:1159.53\n",
      "[800]\ttrain-mae:1119.55\teval-mae:1157.17\n",
      "[850]\ttrain-mae:1116.31\teval-mae:1155.3\n",
      "[900]\ttrain-mae:1113.32\teval-mae:1153.61\n",
      "[950]\ttrain-mae:1110.7\teval-mae:1152.32\n",
      "[1000]\ttrain-mae:1108.27\teval-mae:1151.16\n",
      "[1050]\ttrain-mae:1106.08\teval-mae:1150.09\n",
      "[1100]\ttrain-mae:1104.11\teval-mae:1149.14\n",
      "[1150]\ttrain-mae:1102.22\teval-mae:1148.3\n",
      "[1200]\ttrain-mae:1100.35\teval-mae:1147.48\n",
      "[1250]\ttrain-mae:1098.62\teval-mae:1146.89\n",
      "[1300]\ttrain-mae:1097.04\teval-mae:1146.23\n",
      "[1350]\ttrain-mae:1095.28\teval-mae:1145.67\n",
      "[1400]\ttrain-mae:1093.75\teval-mae:1145.04\n",
      "[1450]\ttrain-mae:1092.28\teval-mae:1144.53\n",
      "[1500]\ttrain-mae:1090.7\teval-mae:1143.92\n",
      "[1550]\ttrain-mae:1089.09\teval-mae:1143.4\n",
      "[1600]\ttrain-mae:1087.79\teval-mae:1142.99\n",
      "[1650]\ttrain-mae:1086.36\teval-mae:1142.62\n",
      "[1700]\ttrain-mae:1084.97\teval-mae:1142.24\n",
      "[1750]\ttrain-mae:1083.57\teval-mae:1141.76\n",
      "[1800]\ttrain-mae:1082.22\teval-mae:1141.32\n",
      "[1850]\ttrain-mae:1080.95\teval-mae:1140.97\n",
      "[1900]\ttrain-mae:1079.74\teval-mae:1140.66\n",
      "[1950]\ttrain-mae:1078.44\teval-mae:1140.4\n",
      "[2000]\ttrain-mae:1077.17\teval-mae:1140.08\n",
      "[2050]\ttrain-mae:1075.96\teval-mae:1139.82\n",
      "[2100]\ttrain-mae:1074.62\teval-mae:1139.53\n",
      "[2150]\ttrain-mae:1073.3\teval-mae:1139.2\n",
      "[2200]\ttrain-mae:1072.11\teval-mae:1139.05\n",
      "[2250]\ttrain-mae:1070.89\teval-mae:1138.83\n",
      "[2300]\ttrain-mae:1069.73\teval-mae:1138.57\n",
      "[2350]\ttrain-mae:1068.62\teval-mae:1138.28\n",
      "[2400]\ttrain-mae:1067.43\teval-mae:1138.11\n",
      "[2450]\ttrain-mae:1066.29\teval-mae:1137.95\n",
      "[2500]\ttrain-mae:1065.11\teval-mae:1137.75\n",
      "[2550]\ttrain-mae:1064.04\teval-mae:1137.62\n",
      "[2600]\ttrain-mae:1062.95\teval-mae:1137.41\n",
      "[2650]\ttrain-mae:1061.71\teval-mae:1137.22\n",
      "[2700]\ttrain-mae:1060.5\teval-mae:1137.05\n",
      "[2750]\ttrain-mae:1059.41\teval-mae:1136.9\n",
      "[2800]\ttrain-mae:1058.29\teval-mae:1136.75\n",
      "[2850]\ttrain-mae:1057.23\teval-mae:1136.58\n",
      "[2900]\ttrain-mae:1056.05\teval-mae:1136.32\n",
      "[2950]\ttrain-mae:1054.95\teval-mae:1136.21\n",
      "[3000]\ttrain-mae:1053.85\teval-mae:1136.13\n",
      "[3050]\ttrain-mae:1052.59\teval-mae:1135.93\n",
      "[3100]\ttrain-mae:1051.56\teval-mae:1135.76\n",
      "[3150]\ttrain-mae:1050.54\teval-mae:1135.64\n",
      "[3200]\ttrain-mae:1049.35\teval-mae:1135.54\n",
      "[3250]\ttrain-mae:1048.39\teval-mae:1135.48\n",
      "[3300]\ttrain-mae:1047.34\teval-mae:1135.47\n",
      "Stopping. Best iteration:\n",
      "[3256]\ttrain-mae:1048.25\teval-mae:1135.45\n",
      "\n",
      "eval-MAE: 1135.453305\n",
      "\n",
      " Fold 2\n",
      "[0]\ttrain-mae:3236.77\teval-mae:3217.56\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2782.15\teval-mae:2761.94\n",
      "[100]\ttrain-mae:2233.38\teval-mae:2211.25\n",
      "[150]\ttrain-mae:1859.91\teval-mae:1837.73\n",
      "[200]\ttrain-mae:1613.39\teval-mae:1594.64\n",
      "[250]\ttrain-mae:1450.53\teval-mae:1435.77\n",
      "[300]\ttrain-mae:1342.69\teval-mae:1332.7\n",
      "[350]\ttrain-mae:1271.27\teval-mae:1265.28\n",
      "[400]\ttrain-mae:1223.84\teval-mae:1221.72\n",
      "[450]\ttrain-mae:1192.09\teval-mae:1193.61\n",
      "[500]\ttrain-mae:1170.43\teval-mae:1175.35\n",
      "[550]\ttrain-mae:1155.33\teval-mae:1163.17\n",
      "[600]\ttrain-mae:1144.64\teval-mae:1155.16\n",
      "[650]\ttrain-mae:1136.56\teval-mae:1149.63\n",
      "[700]\ttrain-mae:1130.57\teval-mae:1145.75\n",
      "[750]\ttrain-mae:1125.74\teval-mae:1143.03\n",
      "[800]\ttrain-mae:1121.89\teval-mae:1140.84\n",
      "[850]\ttrain-mae:1118.39\teval-mae:1139.13\n",
      "[900]\ttrain-mae:1115.36\teval-mae:1137.71\n",
      "[950]\ttrain-mae:1112.63\teval-mae:1136.47\n",
      "[1000]\ttrain-mae:1110.19\teval-mae:1135.47\n",
      "[1050]\ttrain-mae:1108\teval-mae:1134.55\n",
      "[1100]\ttrain-mae:1106.01\teval-mae:1133.69\n",
      "[1150]\ttrain-mae:1104.07\teval-mae:1133\n",
      "[1200]\ttrain-mae:1102.12\teval-mae:1132.35\n",
      "[1250]\ttrain-mae:1100.37\teval-mae:1131.74\n",
      "[1300]\ttrain-mae:1098.58\teval-mae:1131.11\n",
      "[1350]\ttrain-mae:1096.82\teval-mae:1130.48\n",
      "[1400]\ttrain-mae:1095.18\teval-mae:1129.9\n",
      "[1450]\ttrain-mae:1093.68\teval-mae:1129.54\n",
      "[1500]\ttrain-mae:1092.23\teval-mae:1129.12\n",
      "[1550]\ttrain-mae:1090.55\teval-mae:1128.57\n",
      "[1600]\ttrain-mae:1089.07\teval-mae:1128.13\n",
      "[1650]\ttrain-mae:1087.61\teval-mae:1127.74\n",
      "[1700]\ttrain-mae:1086.26\teval-mae:1127.4\n",
      "[1750]\ttrain-mae:1084.9\teval-mae:1127.08\n",
      "[1800]\ttrain-mae:1083.56\teval-mae:1126.86\n",
      "[1850]\ttrain-mae:1082.2\teval-mae:1126.58\n",
      "[1900]\ttrain-mae:1080.86\teval-mae:1126.27\n",
      "[1950]\ttrain-mae:1079.57\teval-mae:1125.97\n",
      "[2000]\ttrain-mae:1078.42\teval-mae:1125.62\n",
      "[2050]\ttrain-mae:1077.16\teval-mae:1125.27\n",
      "[2100]\ttrain-mae:1075.85\teval-mae:1125.02\n",
      "[2150]\ttrain-mae:1074.45\teval-mae:1124.83\n",
      "[2200]\ttrain-mae:1073.18\teval-mae:1124.56\n",
      "[2250]\ttrain-mae:1071.92\teval-mae:1124.27\n",
      "[2300]\ttrain-mae:1070.67\teval-mae:1123.99\n",
      "[2350]\ttrain-mae:1069.5\teval-mae:1123.71\n",
      "[2400]\ttrain-mae:1068.43\teval-mae:1123.51\n",
      "[2450]\ttrain-mae:1067.15\teval-mae:1123.28\n",
      "[2500]\ttrain-mae:1065.98\teval-mae:1123.12\n",
      "[2550]\ttrain-mae:1064.81\teval-mae:1122.86\n",
      "[2600]\ttrain-mae:1063.7\teval-mae:1122.73\n",
      "[2650]\ttrain-mae:1062.44\teval-mae:1122.56\n",
      "[2700]\ttrain-mae:1061.19\teval-mae:1122.42\n",
      "[2750]\ttrain-mae:1060.02\teval-mae:1122.22\n",
      "[2800]\ttrain-mae:1058.87\teval-mae:1122.06\n",
      "[2850]\ttrain-mae:1057.7\teval-mae:1121.95\n",
      "[2900]\ttrain-mae:1056.56\teval-mae:1121.83\n",
      "[2950]\ttrain-mae:1055.54\teval-mae:1121.71\n",
      "[3000]\ttrain-mae:1054.49\teval-mae:1121.54\n",
      "[3050]\ttrain-mae:1053.3\teval-mae:1121.38\n",
      "[3100]\ttrain-mae:1052.33\teval-mae:1121.27\n",
      "[3150]\ttrain-mae:1051.22\teval-mae:1121.11\n",
      "[3200]\ttrain-mae:1050.13\teval-mae:1121.02\n",
      "[3250]\ttrain-mae:1049.19\teval-mae:1120.89\n",
      "[3300]\ttrain-mae:1048.15\teval-mae:1120.83\n",
      "[3350]\ttrain-mae:1047.09\teval-mae:1120.69\n",
      "[3400]\ttrain-mae:1046.01\teval-mae:1120.59\n",
      "[3450]\ttrain-mae:1044.93\teval-mae:1120.49\n",
      "[3500]\ttrain-mae:1043.88\teval-mae:1120.39\n",
      "[3550]\ttrain-mae:1042.88\teval-mae:1120.31\n",
      "[3600]\ttrain-mae:1041.99\teval-mae:1120.2\n",
      "[3650]\ttrain-mae:1041.08\teval-mae:1120.08\n",
      "[3700]\ttrain-mae:1040.02\teval-mae:1120.05\n",
      "[3750]\ttrain-mae:1038.96\teval-mae:1119.98\n",
      "[3800]\ttrain-mae:1037.92\teval-mae:1119.91\n",
      "[3850]\ttrain-mae:1036.97\teval-mae:1119.83\n",
      "[3900]\ttrain-mae:1035.96\teval-mae:1119.73\n",
      "[3950]\ttrain-mae:1034.93\teval-mae:1119.59\n",
      "[4000]\ttrain-mae:1033.95\teval-mae:1119.53\n",
      "[4050]\ttrain-mae:1033.04\teval-mae:1119.51\n",
      "[4100]\ttrain-mae:1031.97\teval-mae:1119.43\n",
      "[4150]\ttrain-mae:1031.02\teval-mae:1119.37\n",
      "[4200]\ttrain-mae:1030.03\teval-mae:1119.31\n",
      "[4250]\ttrain-mae:1029.04\teval-mae:1119.22\n",
      "[4300]\ttrain-mae:1028.09\teval-mae:1119.2\n",
      "[4350]\ttrain-mae:1027.13\teval-mae:1119.1\n",
      "[4400]\ttrain-mae:1026.19\teval-mae:1119.06\n",
      "[4450]\ttrain-mae:1025.25\teval-mae:1118.96\n",
      "Stopping. Best iteration:\n",
      "[4446]\ttrain-mae:1025.35\teval-mae:1118.93\n",
      "\n",
      "eval-MAE: 1118.928713\n",
      "\n",
      " Fold 3\n",
      "[0]\ttrain-mae:3234.06\teval-mae:3241.94\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2779.43\teval-mae:2788.26\n",
      "[100]\ttrain-mae:2230.46\teval-mae:2241.48\n",
      "[150]\ttrain-mae:1856.96\teval-mae:1871.16\n",
      "[200]\ttrain-mae:1610.77\teval-mae:1627.67\n",
      "[250]\ttrain-mae:1448.18\teval-mae:1466.46\n",
      "[300]\ttrain-mae:1340.73\teval-mae:1360.4\n",
      "[350]\ttrain-mae:1269.31\teval-mae:1290.96\n",
      "[400]\ttrain-mae:1221.98\teval-mae:1245.86\n",
      "[450]\ttrain-mae:1190.32\teval-mae:1215.95\n",
      "[500]\ttrain-mae:1168.7\teval-mae:1195.89\n",
      "[550]\ttrain-mae:1153.77\teval-mae:1182.42\n",
      "[600]\ttrain-mae:1143.06\teval-mae:1173.21\n",
      "[650]\ttrain-mae:1135.09\teval-mae:1166.85\n",
      "[700]\ttrain-mae:1129.09\teval-mae:1162.32\n",
      "[750]\ttrain-mae:1124.19\teval-mae:1158.83\n",
      "[800]\ttrain-mae:1120.37\teval-mae:1156.24\n",
      "[850]\ttrain-mae:1116.95\teval-mae:1154.15\n",
      "[900]\ttrain-mae:1113.84\teval-mae:1152.48\n",
      "[950]\ttrain-mae:1111.15\teval-mae:1151.07\n",
      "[1000]\ttrain-mae:1108.8\teval-mae:1149.9\n",
      "[1050]\ttrain-mae:1106.61\teval-mae:1148.92\n",
      "[1100]\ttrain-mae:1104.71\teval-mae:1147.98\n",
      "[1150]\ttrain-mae:1102.87\teval-mae:1147.19\n",
      "[1200]\ttrain-mae:1101.03\teval-mae:1146.44\n",
      "[1250]\ttrain-mae:1099.13\teval-mae:1145.71\n",
      "[1300]\ttrain-mae:1097.45\teval-mae:1145.03\n",
      "[1350]\ttrain-mae:1095.73\teval-mae:1144.43\n",
      "[1400]\ttrain-mae:1094.08\teval-mae:1143.94\n",
      "[1450]\ttrain-mae:1092.48\teval-mae:1143.45\n",
      "[1500]\ttrain-mae:1090.93\teval-mae:1142.92\n",
      "[1550]\ttrain-mae:1089.41\teval-mae:1142.51\n",
      "[1600]\ttrain-mae:1088.11\teval-mae:1142.2\n",
      "[1650]\ttrain-mae:1086.63\teval-mae:1141.81\n",
      "[1700]\ttrain-mae:1085.11\teval-mae:1141.4\n",
      "[1750]\ttrain-mae:1083.79\teval-mae:1141.08\n",
      "[1800]\ttrain-mae:1082.42\teval-mae:1140.75\n",
      "[1850]\ttrain-mae:1081.17\teval-mae:1140.39\n",
      "[1900]\ttrain-mae:1079.84\teval-mae:1140.18\n",
      "[1950]\ttrain-mae:1078.58\teval-mae:1139.89\n",
      "[2000]\ttrain-mae:1077.44\teval-mae:1139.64\n",
      "[2050]\ttrain-mae:1076.2\teval-mae:1139.4\n",
      "[2100]\ttrain-mae:1074.84\teval-mae:1139.15\n",
      "[2150]\ttrain-mae:1073.52\teval-mae:1138.84\n",
      "[2200]\ttrain-mae:1072.3\teval-mae:1138.64\n",
      "[2250]\ttrain-mae:1071.16\teval-mae:1138.4\n",
      "[2300]\ttrain-mae:1069.9\teval-mae:1138.19\n",
      "[2350]\ttrain-mae:1068.71\teval-mae:1137.98\n",
      "[2400]\ttrain-mae:1067.53\teval-mae:1137.81\n",
      "[2450]\ttrain-mae:1066.26\teval-mae:1137.57\n",
      "[2500]\ttrain-mae:1065.04\teval-mae:1137.4\n",
      "[2550]\ttrain-mae:1063.81\teval-mae:1137.22\n",
      "[2600]\ttrain-mae:1062.72\teval-mae:1136.99\n",
      "[2650]\ttrain-mae:1061.49\teval-mae:1136.84\n",
      "[2700]\ttrain-mae:1060.34\teval-mae:1136.78\n",
      "[2750]\ttrain-mae:1059.23\teval-mae:1136.63\n",
      "[2800]\ttrain-mae:1058.17\teval-mae:1136.49\n",
      "[2850]\ttrain-mae:1057.02\teval-mae:1136.28\n",
      "[2900]\ttrain-mae:1055.94\teval-mae:1136.14\n",
      "[2950]\ttrain-mae:1054.9\teval-mae:1136.04\n",
      "[3000]\ttrain-mae:1053.84\teval-mae:1135.89\n",
      "[3050]\ttrain-mae:1052.7\teval-mae:1135.76\n",
      "[3100]\ttrain-mae:1051.66\teval-mae:1135.6\n",
      "[3150]\ttrain-mae:1050.65\teval-mae:1135.46\n",
      "[3200]\ttrain-mae:1049.49\teval-mae:1135.32\n",
      "[3250]\ttrain-mae:1048.5\teval-mae:1135.23\n",
      "[3300]\ttrain-mae:1047.4\teval-mae:1135.09\n",
      "[3350]\ttrain-mae:1046.31\teval-mae:1135.06\n",
      "[3400]\ttrain-mae:1045.24\teval-mae:1135.05\n",
      "[3450]\ttrain-mae:1044.27\teval-mae:1134.98\n",
      "[3500]\ttrain-mae:1043.31\teval-mae:1134.97\n",
      "Stopping. Best iteration:\n",
      "[3468]\ttrain-mae:1043.92\teval-mae:1134.95\n",
      "\n",
      "eval-MAE: 1134.946064\n",
      "\n",
      " Fold 4\n",
      "[0]\ttrain-mae:3235.45\teval-mae:3229.43\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2781.79\teval-mae:2774.93\n",
      "[100]\ttrain-mae:2233.41\teval-mae:2225.06\n",
      "[150]\ttrain-mae:1860.03\teval-mae:1852.27\n",
      "[200]\ttrain-mae:1613.78\teval-mae:1607.68\n",
      "[250]\ttrain-mae:1450.85\teval-mae:1447.67\n",
      "[300]\ttrain-mae:1343.21\teval-mae:1342.72\n",
      "[350]\ttrain-mae:1271.95\teval-mae:1273.3\n",
      "[400]\ttrain-mae:1224.7\teval-mae:1227.74\n",
      "[450]\ttrain-mae:1193.01\teval-mae:1197.42\n",
      "[500]\ttrain-mae:1171.24\teval-mae:1176.82\n",
      "[550]\ttrain-mae:1156.15\teval-mae:1162.91\n",
      "[600]\ttrain-mae:1145.5\teval-mae:1153.18\n",
      "[650]\ttrain-mae:1137.56\teval-mae:1146.51\n",
      "[700]\ttrain-mae:1131.58\teval-mae:1141.79\n",
      "[750]\ttrain-mae:1126.73\teval-mae:1138.05\n",
      "[800]\ttrain-mae:1122.93\teval-mae:1135.24\n",
      "[850]\ttrain-mae:1119.63\teval-mae:1132.97\n",
      "[900]\ttrain-mae:1116.58\teval-mae:1131.14\n",
      "[950]\ttrain-mae:1113.94\teval-mae:1129.64\n",
      "[1000]\ttrain-mae:1111.58\teval-mae:1128.47\n",
      "[1050]\ttrain-mae:1109.36\teval-mae:1127.39\n",
      "[1100]\ttrain-mae:1107.45\teval-mae:1126.43\n",
      "[1150]\ttrain-mae:1105.45\teval-mae:1125.53\n",
      "[1200]\ttrain-mae:1103.67\teval-mae:1124.92\n",
      "[1250]\ttrain-mae:1101.86\teval-mae:1124.17\n",
      "[1300]\ttrain-mae:1100.17\teval-mae:1123.58\n",
      "[1350]\ttrain-mae:1098.36\teval-mae:1123.01\n",
      "[1400]\ttrain-mae:1096.81\teval-mae:1122.49\n",
      "[1450]\ttrain-mae:1095.14\teval-mae:1121.92\n",
      "[1500]\ttrain-mae:1093.73\teval-mae:1121.52\n",
      "[1550]\ttrain-mae:1092.2\teval-mae:1121.14\n",
      "[1600]\ttrain-mae:1090.72\teval-mae:1120.69\n",
      "[1650]\ttrain-mae:1089.28\teval-mae:1120.31\n",
      "[1700]\ttrain-mae:1087.83\teval-mae:1119.94\n",
      "[1750]\ttrain-mae:1086.44\teval-mae:1119.63\n",
      "[1800]\ttrain-mae:1085.06\teval-mae:1119.26\n",
      "[1850]\ttrain-mae:1083.78\teval-mae:1119.01\n",
      "[1900]\ttrain-mae:1082.45\teval-mae:1118.65\n",
      "[1950]\ttrain-mae:1081.21\teval-mae:1118.41\n",
      "[2000]\ttrain-mae:1079.87\teval-mae:1118.1\n",
      "[2050]\ttrain-mae:1078.67\teval-mae:1117.82\n",
      "[2100]\ttrain-mae:1077.41\teval-mae:1117.64\n",
      "[2150]\ttrain-mae:1076.09\teval-mae:1117.4\n",
      "[2200]\ttrain-mae:1074.82\teval-mae:1117.15\n",
      "[2250]\ttrain-mae:1073.64\teval-mae:1117.04\n",
      "[2300]\ttrain-mae:1072.35\teval-mae:1116.79\n",
      "[2350]\ttrain-mae:1071.03\teval-mae:1116.64\n",
      "[2400]\ttrain-mae:1069.91\teval-mae:1116.43\n",
      "[2450]\ttrain-mae:1068.64\teval-mae:1116.23\n",
      "[2500]\ttrain-mae:1067.41\teval-mae:1116.16\n",
      "[2550]\ttrain-mae:1066.23\teval-mae:1116.01\n",
      "[2600]\ttrain-mae:1065.09\teval-mae:1115.89\n",
      "[2650]\ttrain-mae:1063.96\teval-mae:1115.78\n",
      "[2700]\ttrain-mae:1062.88\teval-mae:1115.73\n",
      "[2750]\ttrain-mae:1061.71\teval-mae:1115.68\n",
      "[2800]\ttrain-mae:1060.59\teval-mae:1115.6\n",
      "[2850]\ttrain-mae:1059.48\teval-mae:1115.53\n",
      "[2900]\ttrain-mae:1058.32\teval-mae:1115.4\n",
      "[2950]\ttrain-mae:1057.23\teval-mae:1115.26\n",
      "[3000]\ttrain-mae:1056.2\teval-mae:1115.14\n",
      "[3050]\ttrain-mae:1055.18\teval-mae:1114.97\n",
      "[3100]\ttrain-mae:1054.07\teval-mae:1114.86\n",
      "[3150]\ttrain-mae:1053.01\teval-mae:1114.73\n",
      "[3200]\ttrain-mae:1051.89\teval-mae:1114.53\n",
      "[3250]\ttrain-mae:1050.9\teval-mae:1114.47\n",
      "[3300]\ttrain-mae:1049.86\teval-mae:1114.43\n",
      "[3350]\ttrain-mae:1048.89\teval-mae:1114.32\n",
      "[3400]\ttrain-mae:1047.8\teval-mae:1114.23\n",
      "Stopping. Best iteration:\n",
      "[3388]\ttrain-mae:1048.07\teval-mae:1114.2\n",
      "\n",
      "eval-MAE: 1114.196552\n",
      "\n",
      " Fold 5\n",
      "[0]\ttrain-mae:3232.5\teval-mae:3255.99\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2778.06\teval-mae:2801.46\n",
      "[100]\ttrain-mae:2229.63\teval-mae:2252.41\n",
      "[150]\ttrain-mae:1856.12\teval-mae:1881.01\n",
      "[200]\ttrain-mae:1610.03\teval-mae:1635.69\n",
      "[250]\ttrain-mae:1447.07\teval-mae:1475.6\n",
      "[300]\ttrain-mae:1339.31\teval-mae:1371.36\n",
      "[350]\ttrain-mae:1267.9\teval-mae:1302.64\n",
      "[400]\ttrain-mae:1220.52\teval-mae:1256.89\n",
      "[450]\ttrain-mae:1188.85\teval-mae:1226.18\n",
      "[500]\ttrain-mae:1167.08\teval-mae:1205.45\n",
      "[550]\ttrain-mae:1152.14\teval-mae:1191.45\n",
      "[600]\ttrain-mae:1141.56\teval-mae:1181.66\n",
      "[650]\ttrain-mae:1133.59\teval-mae:1174.68\n",
      "[700]\ttrain-mae:1127.75\teval-mae:1169.61\n",
      "[750]\ttrain-mae:1122.93\teval-mae:1165.76\n",
      "[800]\ttrain-mae:1119.16\teval-mae:1162.74\n",
      "[850]\ttrain-mae:1115.87\teval-mae:1160.32\n",
      "[900]\ttrain-mae:1112.95\teval-mae:1158.34\n",
      "[950]\ttrain-mae:1110.32\teval-mae:1156.59\n",
      "[1000]\ttrain-mae:1107.78\teval-mae:1155.1\n",
      "[1050]\ttrain-mae:1105.62\teval-mae:1153.95\n",
      "[1100]\ttrain-mae:1103.53\teval-mae:1152.83\n",
      "[1150]\ttrain-mae:1101.62\teval-mae:1151.87\n",
      "[1200]\ttrain-mae:1099.83\teval-mae:1151.05\n",
      "[1250]\ttrain-mae:1098.09\teval-mae:1150.2\n",
      "[1300]\ttrain-mae:1096.48\teval-mae:1149.52\n",
      "[1350]\ttrain-mae:1094.64\teval-mae:1148.74\n",
      "[1400]\ttrain-mae:1093.18\teval-mae:1148.12\n",
      "[1450]\ttrain-mae:1091.59\teval-mae:1147.42\n",
      "[1500]\ttrain-mae:1090.09\teval-mae:1146.86\n",
      "[1550]\ttrain-mae:1088.63\teval-mae:1146.36\n",
      "[1600]\ttrain-mae:1087.17\teval-mae:1145.9\n",
      "[1650]\ttrain-mae:1085.79\teval-mae:1145.45\n",
      "[1700]\ttrain-mae:1084.32\teval-mae:1144.97\n",
      "[1750]\ttrain-mae:1082.89\teval-mae:1144.54\n",
      "[1800]\ttrain-mae:1081.44\teval-mae:1144.07\n",
      "[1850]\ttrain-mae:1080.14\teval-mae:1143.71\n",
      "[1900]\ttrain-mae:1078.74\teval-mae:1143.36\n",
      "[1950]\ttrain-mae:1077.46\teval-mae:1143.03\n",
      "[2000]\ttrain-mae:1076.26\teval-mae:1142.69\n",
      "[2050]\ttrain-mae:1074.98\teval-mae:1142.44\n",
      "[2100]\ttrain-mae:1073.67\teval-mae:1142.16\n",
      "[2150]\ttrain-mae:1072.32\teval-mae:1141.87\n",
      "[2200]\ttrain-mae:1071.06\teval-mae:1141.5\n",
      "[2250]\ttrain-mae:1069.86\teval-mae:1141.16\n",
      "[2300]\ttrain-mae:1068.59\teval-mae:1140.86\n",
      "[2350]\ttrain-mae:1067.4\teval-mae:1140.57\n",
      "[2400]\ttrain-mae:1066.23\teval-mae:1140.36\n",
      "[2450]\ttrain-mae:1065.04\teval-mae:1140.07\n",
      "[2500]\ttrain-mae:1063.97\teval-mae:1139.88\n",
      "[2550]\ttrain-mae:1062.8\teval-mae:1139.65\n",
      "[2600]\ttrain-mae:1061.72\teval-mae:1139.52\n",
      "[2650]\ttrain-mae:1060.63\teval-mae:1139.36\n",
      "[2700]\ttrain-mae:1059.42\teval-mae:1139.14\n",
      "[2750]\ttrain-mae:1058.35\teval-mae:1138.92\n",
      "[2800]\ttrain-mae:1057.17\teval-mae:1138.77\n",
      "[2850]\ttrain-mae:1056.02\teval-mae:1138.54\n",
      "[2900]\ttrain-mae:1054.85\teval-mae:1138.24\n",
      "[2950]\ttrain-mae:1053.72\teval-mae:1138.06\n",
      "[3000]\ttrain-mae:1052.63\teval-mae:1137.94\n",
      "[3050]\ttrain-mae:1051.68\teval-mae:1137.72\n",
      "[3100]\ttrain-mae:1050.61\teval-mae:1137.58\n",
      "[3150]\ttrain-mae:1049.52\teval-mae:1137.41\n",
      "[3200]\ttrain-mae:1048.4\teval-mae:1137.29\n",
      "[3250]\ttrain-mae:1047.41\teval-mae:1137.25\n",
      "[3300]\ttrain-mae:1046.3\teval-mae:1137.1\n",
      "[3350]\ttrain-mae:1045.22\teval-mae:1136.99\n",
      "[3400]\ttrain-mae:1044.15\teval-mae:1136.9\n",
      "[3450]\ttrain-mae:1043.19\teval-mae:1136.81\n",
      "[3500]\ttrain-mae:1042.23\teval-mae:1136.76\n",
      "[3550]\ttrain-mae:1041.13\teval-mae:1136.63\n",
      "[3600]\ttrain-mae:1040.17\teval-mae:1136.57\n",
      "[3650]\ttrain-mae:1039.19\teval-mae:1136.48\n",
      "[3700]\ttrain-mae:1038.02\teval-mae:1136.35\n",
      "[3750]\ttrain-mae:1037.04\teval-mae:1136.31\n",
      "[3800]\ttrain-mae:1036.03\teval-mae:1136.2\n",
      "[3850]\ttrain-mae:1034.95\teval-mae:1136.1\n",
      "[3900]\ttrain-mae:1033.99\teval-mae:1136.01\n",
      "[3950]\ttrain-mae:1033\teval-mae:1135.94\n",
      "[4000]\ttrain-mae:1031.96\teval-mae:1135.87\n",
      "[4050]\ttrain-mae:1031\teval-mae:1135.79\n",
      "[4100]\ttrain-mae:1030.05\teval-mae:1135.71\n",
      "[4150]\ttrain-mae:1029.08\teval-mae:1135.69\n",
      "[4200]\ttrain-mae:1028.15\teval-mae:1135.66\n",
      "[4250]\ttrain-mae:1027.16\teval-mae:1135.62\n",
      "[4300]\ttrain-mae:1026.23\teval-mae:1135.51\n",
      "[4350]\ttrain-mae:1025.29\teval-mae:1135.41\n",
      "[4400]\ttrain-mae:1024.34\teval-mae:1135.35\n",
      "[4450]\ttrain-mae:1023.33\teval-mae:1135.31\n",
      "[4500]\ttrain-mae:1022.4\teval-mae:1135.27\n",
      "[4550]\ttrain-mae:1021.47\teval-mae:1135.2\n",
      "[4600]\ttrain-mae:1020.63\teval-mae:1135.12\n",
      "[4650]\ttrain-mae:1019.76\teval-mae:1135.13\n",
      "Stopping. Best iteration:\n",
      "[4601]\ttrain-mae:1020.61\teval-mae:1135.12\n",
      "\n",
      "eval-MAE: 1135.119101\n",
      "\n",
      " Fold 6\n",
      "[0]\ttrain-mae:3234.91\teval-mae:3234.27\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2780.28\teval-mae:2782.14\n",
      "[100]\ttrain-mae:2231.72\teval-mae:2237.12\n",
      "[150]\ttrain-mae:1858.46\teval-mae:1866.87\n",
      "[200]\ttrain-mae:1612.02\teval-mae:1624.54\n",
      "[250]\ttrain-mae:1449.18\teval-mae:1465.1\n",
      "[300]\ttrain-mae:1341.53\teval-mae:1359.49\n",
      "[350]\ttrain-mae:1270.05\teval-mae:1290.64\n",
      "[400]\ttrain-mae:1222.78\teval-mae:1245.6\n",
      "[450]\ttrain-mae:1190.78\teval-mae:1215.47\n",
      "[500]\ttrain-mae:1168.95\teval-mae:1195.59\n",
      "[550]\ttrain-mae:1153.81\teval-mae:1182.41\n",
      "[600]\ttrain-mae:1143.17\teval-mae:1173.39\n",
      "[650]\ttrain-mae:1135.35\teval-mae:1167.23\n",
      "[700]\ttrain-mae:1129.38\teval-mae:1162.68\n",
      "[750]\ttrain-mae:1124.47\teval-mae:1159.35\n",
      "[800]\ttrain-mae:1120.56\teval-mae:1156.71\n",
      "[850]\ttrain-mae:1117.1\teval-mae:1154.54\n",
      "[900]\ttrain-mae:1114.1\teval-mae:1152.84\n",
      "[950]\ttrain-mae:1111.43\teval-mae:1151.42\n",
      "[1000]\ttrain-mae:1108.9\teval-mae:1150.18\n",
      "[1050]\ttrain-mae:1106.6\teval-mae:1149.1\n",
      "[1100]\ttrain-mae:1104.56\teval-mae:1148.14\n",
      "[1150]\ttrain-mae:1102.62\teval-mae:1147.35\n",
      "[1200]\ttrain-mae:1100.95\teval-mae:1146.7\n",
      "[1250]\ttrain-mae:1099.14\teval-mae:1145.95\n",
      "[1300]\ttrain-mae:1097.53\teval-mae:1145.31\n",
      "[1350]\ttrain-mae:1095.85\teval-mae:1144.74\n",
      "[1400]\ttrain-mae:1094.25\teval-mae:1144.24\n",
      "[1450]\ttrain-mae:1092.7\teval-mae:1143.65\n",
      "[1500]\ttrain-mae:1091.17\teval-mae:1143.21\n",
      "[1550]\ttrain-mae:1089.64\teval-mae:1142.76\n",
      "[1600]\ttrain-mae:1088.21\teval-mae:1142.32\n",
      "[1650]\ttrain-mae:1086.71\teval-mae:1141.82\n",
      "[1700]\ttrain-mae:1085.2\teval-mae:1141.43\n",
      "[1750]\ttrain-mae:1083.84\teval-mae:1141.04\n",
      "[1800]\ttrain-mae:1082.43\teval-mae:1140.69\n",
      "[1850]\ttrain-mae:1081.12\teval-mae:1140.28\n",
      "[1900]\ttrain-mae:1079.79\teval-mae:1139.91\n",
      "[1950]\ttrain-mae:1078.49\teval-mae:1139.58\n",
      "[2000]\ttrain-mae:1077.25\teval-mae:1139.26\n",
      "[2050]\ttrain-mae:1076\teval-mae:1139.04\n",
      "[2100]\ttrain-mae:1074.67\teval-mae:1138.73\n",
      "[2150]\ttrain-mae:1073.43\teval-mae:1138.43\n",
      "[2200]\ttrain-mae:1072.21\teval-mae:1138.14\n",
      "[2250]\ttrain-mae:1070.97\teval-mae:1137.79\n",
      "[2300]\ttrain-mae:1069.79\teval-mae:1137.54\n",
      "[2350]\ttrain-mae:1068.58\teval-mae:1137.29\n",
      "[2400]\ttrain-mae:1067.44\teval-mae:1137.06\n",
      "[2450]\ttrain-mae:1066.3\teval-mae:1136.85\n",
      "[2500]\ttrain-mae:1065.05\teval-mae:1136.59\n",
      "[2550]\ttrain-mae:1063.84\teval-mae:1136.45\n",
      "[2600]\ttrain-mae:1062.68\teval-mae:1136.3\n",
      "[2650]\ttrain-mae:1061.48\teval-mae:1136.16\n",
      "[2700]\ttrain-mae:1060.33\teval-mae:1136.01\n",
      "[2750]\ttrain-mae:1059.28\teval-mae:1135.81\n",
      "[2800]\ttrain-mae:1058.19\teval-mae:1135.64\n",
      "[2850]\ttrain-mae:1057\teval-mae:1135.38\n",
      "[2900]\ttrain-mae:1055.86\teval-mae:1135.26\n",
      "[2950]\ttrain-mae:1054.72\teval-mae:1135.14\n",
      "[3000]\ttrain-mae:1053.62\teval-mae:1135.07\n",
      "[3050]\ttrain-mae:1052.58\teval-mae:1134.92\n",
      "[3100]\ttrain-mae:1051.51\teval-mae:1134.8\n",
      "[3150]\ttrain-mae:1050.43\teval-mae:1134.72\n",
      "[3200]\ttrain-mae:1049.22\teval-mae:1134.6\n",
      "[3250]\ttrain-mae:1048.18\teval-mae:1134.47\n",
      "[3300]\ttrain-mae:1047.09\teval-mae:1134.36\n",
      "[3350]\ttrain-mae:1045.98\teval-mae:1134.26\n",
      "[3400]\ttrain-mae:1044.91\teval-mae:1134.11\n",
      "[3450]\ttrain-mae:1043.95\teval-mae:1133.98\n",
      "[3500]\ttrain-mae:1042.92\teval-mae:1133.88\n",
      "[3550]\ttrain-mae:1041.83\teval-mae:1133.73\n",
      "[3600]\ttrain-mae:1040.85\teval-mae:1133.64\n",
      "[3650]\ttrain-mae:1039.89\teval-mae:1133.49\n",
      "[3700]\ttrain-mae:1038.88\teval-mae:1133.38\n",
      "[3750]\ttrain-mae:1037.95\teval-mae:1133.34\n",
      "[3800]\ttrain-mae:1036.95\teval-mae:1133.32\n",
      "[3850]\ttrain-mae:1035.93\teval-mae:1133.16\n",
      "[3900]\ttrain-mae:1035.01\teval-mae:1133.07\n",
      "[3950]\ttrain-mae:1033.99\teval-mae:1132.96\n",
      "[4000]\ttrain-mae:1032.92\teval-mae:1132.84\n",
      "[4050]\ttrain-mae:1031.99\teval-mae:1132.78\n",
      "[4100]\ttrain-mae:1030.93\teval-mae:1132.75\n",
      "[4150]\ttrain-mae:1029.97\teval-mae:1132.66\n",
      "[4200]\ttrain-mae:1029.06\teval-mae:1132.56\n",
      "[4250]\ttrain-mae:1028.05\teval-mae:1132.54\n",
      "[4300]\ttrain-mae:1027.16\teval-mae:1132.45\n",
      "[4350]\ttrain-mae:1026.18\teval-mae:1132.4\n",
      "[4400]\ttrain-mae:1025.24\teval-mae:1132.29\n",
      "[4450]\ttrain-mae:1024.28\teval-mae:1132.29\n",
      "[4500]\ttrain-mae:1023.33\teval-mae:1132.22\n",
      "[4550]\ttrain-mae:1022.35\teval-mae:1132.11\n",
      "[4600]\ttrain-mae:1021.52\teval-mae:1132.04\n",
      "[4650]\ttrain-mae:1020.61\teval-mae:1131.97\n",
      "[4700]\ttrain-mae:1019.66\teval-mae:1131.91\n",
      "[4750]\ttrain-mae:1018.66\teval-mae:1131.81\n",
      "[4800]\ttrain-mae:1017.78\teval-mae:1131.77\n",
      "[4850]\ttrain-mae:1016.83\teval-mae:1131.77\n",
      "Stopping. Best iteration:\n",
      "[4822]\ttrain-mae:1017.37\teval-mae:1131.72\n",
      "\n",
      "eval-MAE: 1131.718592\n",
      "\n",
      " Fold 7\n",
      "[0]\ttrain-mae:3232.7\teval-mae:3254.19\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2778.25\teval-mae:2797.92\n",
      "[100]\ttrain-mae:2229.57\teval-mae:2246.94\n",
      "[150]\ttrain-mae:1856.69\teval-mae:1872.67\n",
      "[200]\ttrain-mae:1610.48\teval-mae:1627.15\n",
      "[250]\ttrain-mae:1447.66\teval-mae:1466.82\n",
      "[300]\ttrain-mae:1340.2\teval-mae:1361.8\n",
      "[350]\ttrain-mae:1268.93\teval-mae:1291.94\n",
      "[400]\ttrain-mae:1221.91\teval-mae:1246.49\n",
      "[450]\ttrain-mae:1190.22\teval-mae:1216.55\n",
      "[500]\ttrain-mae:1168.67\teval-mae:1196.41\n",
      "[550]\ttrain-mae:1153.68\teval-mae:1182.77\n",
      "[600]\ttrain-mae:1143.07\teval-mae:1173.31\n",
      "[650]\ttrain-mae:1135.05\teval-mae:1166.5\n",
      "[700]\ttrain-mae:1129.28\teval-mae:1161.68\n",
      "[750]\ttrain-mae:1124.35\teval-mae:1158.02\n",
      "[800]\ttrain-mae:1120.33\teval-mae:1155.19\n",
      "[850]\ttrain-mae:1117.01\teval-mae:1153.03\n",
      "[900]\ttrain-mae:1113.97\teval-mae:1151.15\n",
      "[950]\ttrain-mae:1111.23\teval-mae:1149.58\n",
      "[1000]\ttrain-mae:1108.76\teval-mae:1148.2\n",
      "[1050]\ttrain-mae:1106.61\teval-mae:1146.98\n",
      "[1100]\ttrain-mae:1104.63\teval-mae:1145.9\n",
      "[1150]\ttrain-mae:1102.57\teval-mae:1144.96\n",
      "[1200]\ttrain-mae:1100.79\teval-mae:1144.16\n",
      "[1250]\ttrain-mae:1098.96\teval-mae:1143.33\n",
      "[1300]\ttrain-mae:1097.32\teval-mae:1142.67\n",
      "[1350]\ttrain-mae:1095.6\teval-mae:1142.1\n",
      "[1400]\ttrain-mae:1094.06\teval-mae:1141.52\n",
      "[1450]\ttrain-mae:1092.36\teval-mae:1140.88\n",
      "[1500]\ttrain-mae:1090.89\teval-mae:1140.39\n",
      "[1550]\ttrain-mae:1089.45\teval-mae:1139.95\n",
      "[1600]\ttrain-mae:1087.89\teval-mae:1139.47\n",
      "[1650]\ttrain-mae:1086.33\teval-mae:1139.01\n",
      "[1700]\ttrain-mae:1084.89\teval-mae:1138.61\n",
      "[1750]\ttrain-mae:1083.5\teval-mae:1138.22\n",
      "[1800]\ttrain-mae:1082.15\teval-mae:1137.86\n",
      "[1850]\ttrain-mae:1080.86\teval-mae:1137.5\n",
      "[1900]\ttrain-mae:1079.6\teval-mae:1137.12\n",
      "[1950]\ttrain-mae:1078.21\teval-mae:1136.77\n",
      "[2000]\ttrain-mae:1076.99\teval-mae:1136.45\n",
      "[2050]\ttrain-mae:1075.71\teval-mae:1136.14\n",
      "[2100]\ttrain-mae:1074.4\teval-mae:1135.86\n",
      "[2150]\ttrain-mae:1073.23\teval-mae:1135.66\n",
      "[2200]\ttrain-mae:1071.91\teval-mae:1135.32\n",
      "[2250]\ttrain-mae:1070.65\teval-mae:1135.03\n",
      "[2300]\ttrain-mae:1069.4\teval-mae:1134.71\n",
      "[2350]\ttrain-mae:1068.23\teval-mae:1134.49\n",
      "[2400]\ttrain-mae:1067.01\teval-mae:1134.25\n",
      "[2450]\ttrain-mae:1065.83\teval-mae:1134.02\n",
      "[2500]\ttrain-mae:1064.65\teval-mae:1133.8\n",
      "[2550]\ttrain-mae:1063.53\teval-mae:1133.58\n",
      "[2600]\ttrain-mae:1062.34\teval-mae:1133.33\n",
      "[2650]\ttrain-mae:1061.21\teval-mae:1133.14\n",
      "[2700]\ttrain-mae:1060.03\teval-mae:1132.92\n",
      "[2750]\ttrain-mae:1058.97\teval-mae:1132.7\n",
      "[2800]\ttrain-mae:1057.76\teval-mae:1132.48\n",
      "[2850]\ttrain-mae:1056.62\teval-mae:1132.32\n",
      "[2900]\ttrain-mae:1055.52\teval-mae:1132.13\n",
      "[2950]\ttrain-mae:1054.43\teval-mae:1132.01\n",
      "[3000]\ttrain-mae:1053.4\teval-mae:1131.83\n",
      "[3050]\ttrain-mae:1052.36\teval-mae:1131.66\n",
      "[3100]\ttrain-mae:1051.3\teval-mae:1131.49\n",
      "[3150]\ttrain-mae:1050.31\teval-mae:1131.32\n",
      "[3200]\ttrain-mae:1049.3\teval-mae:1131.14\n",
      "[3250]\ttrain-mae:1048.31\teval-mae:1131\n",
      "[3300]\ttrain-mae:1047.24\teval-mae:1130.82\n",
      "[3350]\ttrain-mae:1046.15\teval-mae:1130.62\n",
      "[3400]\ttrain-mae:1045.16\teval-mae:1130.54\n",
      "[3450]\ttrain-mae:1044.15\teval-mae:1130.42\n",
      "[3500]\ttrain-mae:1043.09\teval-mae:1130.34\n",
      "[3550]\ttrain-mae:1042.06\teval-mae:1130.2\n",
      "[3600]\ttrain-mae:1041.08\teval-mae:1130.06\n",
      "[3650]\ttrain-mae:1040.11\teval-mae:1130.01\n",
      "[3700]\ttrain-mae:1039.17\teval-mae:1129.95\n",
      "[3750]\ttrain-mae:1038.22\teval-mae:1129.85\n",
      "[3800]\ttrain-mae:1037.21\teval-mae:1129.85\n",
      "[3850]\ttrain-mae:1036.19\teval-mae:1129.77\n",
      "[3900]\ttrain-mae:1035.22\teval-mae:1129.66\n",
      "[3950]\ttrain-mae:1034.17\teval-mae:1129.58\n",
      "[4000]\ttrain-mae:1033.16\teval-mae:1129.54\n",
      "[4050]\ttrain-mae:1032.2\teval-mae:1129.52\n",
      "[4100]\ttrain-mae:1031.22\teval-mae:1129.44\n",
      "[4150]\ttrain-mae:1030.3\teval-mae:1129.38\n",
      "[4200]\ttrain-mae:1029.43\teval-mae:1129.34\n",
      "[4250]\ttrain-mae:1028.39\teval-mae:1129.25\n",
      "[4300]\ttrain-mae:1027.53\teval-mae:1129.21\n",
      "[4350]\ttrain-mae:1026.52\teval-mae:1129.16\n",
      "Stopping. Best iteration:\n",
      "[4341]\ttrain-mae:1026.72\teval-mae:1129.14\n",
      "\n",
      "eval-MAE: 1129.137480\n",
      "\n",
      " Fold 8\n",
      "[0]\ttrain-mae:3235.96\teval-mae:3224.89\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2781.2\teval-mae:2771.98\n",
      "[100]\ttrain-mae:2232.42\teval-mae:2226.57\n",
      "[150]\ttrain-mae:1858.43\teval-mae:1856.96\n",
      "[200]\ttrain-mae:1611.89\teval-mae:1615.38\n",
      "[250]\ttrain-mae:1449.02\teval-mae:1456.18\n",
      "[300]\ttrain-mae:1341.22\teval-mae:1352.06\n",
      "[350]\ttrain-mae:1269.76\teval-mae:1283.39\n",
      "[400]\ttrain-mae:1222.43\teval-mae:1238.8\n",
      "[450]\ttrain-mae:1190.7\teval-mae:1209.8\n",
      "[500]\ttrain-mae:1169.01\teval-mae:1190.19\n",
      "[550]\ttrain-mae:1153.77\teval-mae:1177.06\n",
      "[600]\ttrain-mae:1143.18\teval-mae:1168.46\n",
      "[650]\ttrain-mae:1135.06\teval-mae:1162.29\n",
      "[700]\ttrain-mae:1129.13\teval-mae:1158.08\n",
      "[750]\ttrain-mae:1124.2\teval-mae:1154.86\n",
      "[800]\ttrain-mae:1120.24\teval-mae:1152.24\n",
      "[850]\ttrain-mae:1116.93\teval-mae:1150.34\n",
      "[900]\ttrain-mae:1113.91\teval-mae:1148.82\n",
      "[950]\ttrain-mae:1111.15\teval-mae:1147.45\n",
      "[1000]\ttrain-mae:1108.82\teval-mae:1146.35\n",
      "[1050]\ttrain-mae:1106.73\teval-mae:1145.48\n",
      "[1100]\ttrain-mae:1104.68\teval-mae:1144.71\n",
      "[1150]\ttrain-mae:1102.63\teval-mae:1143.84\n",
      "[1200]\ttrain-mae:1100.9\teval-mae:1143.15\n",
      "[1250]\ttrain-mae:1099.09\teval-mae:1142.52\n",
      "[1300]\ttrain-mae:1097.4\teval-mae:1142\n",
      "[1350]\ttrain-mae:1095.64\teval-mae:1141.49\n",
      "[1400]\ttrain-mae:1094.03\teval-mae:1141\n",
      "[1450]\ttrain-mae:1092.39\teval-mae:1140.5\n",
      "[1500]\ttrain-mae:1090.89\teval-mae:1140.03\n",
      "[1550]\ttrain-mae:1089.31\teval-mae:1139.55\n",
      "[1600]\ttrain-mae:1087.95\teval-mae:1139.19\n",
      "[1650]\ttrain-mae:1086.52\teval-mae:1138.79\n",
      "[1700]\ttrain-mae:1084.96\teval-mae:1138.41\n",
      "[1750]\ttrain-mae:1083.48\teval-mae:1138.08\n",
      "[1800]\ttrain-mae:1082.19\teval-mae:1137.76\n",
      "[1850]\ttrain-mae:1080.9\teval-mae:1137.51\n",
      "[1900]\ttrain-mae:1079.63\teval-mae:1137.3\n",
      "[1950]\ttrain-mae:1078.42\teval-mae:1137\n",
      "[2000]\ttrain-mae:1077.12\teval-mae:1136.72\n",
      "[2050]\ttrain-mae:1075.82\teval-mae:1136.35\n",
      "[2100]\ttrain-mae:1074.48\teval-mae:1136.14\n",
      "[2150]\ttrain-mae:1073.24\teval-mae:1135.92\n",
      "[2200]\ttrain-mae:1072.02\teval-mae:1135.63\n",
      "[2250]\ttrain-mae:1070.8\teval-mae:1135.4\n",
      "[2300]\ttrain-mae:1069.57\teval-mae:1135.18\n",
      "[2350]\ttrain-mae:1068.45\teval-mae:1135.01\n",
      "[2400]\ttrain-mae:1067.3\teval-mae:1134.84\n",
      "[2450]\ttrain-mae:1066.06\teval-mae:1134.54\n",
      "[2500]\ttrain-mae:1064.97\teval-mae:1134.42\n",
      "[2550]\ttrain-mae:1063.82\teval-mae:1134.23\n",
      "[2600]\ttrain-mae:1062.58\teval-mae:1134.11\n",
      "[2650]\ttrain-mae:1061.4\teval-mae:1133.92\n",
      "[2700]\ttrain-mae:1060.16\teval-mae:1133.74\n",
      "[2750]\ttrain-mae:1058.99\teval-mae:1133.57\n",
      "[2800]\ttrain-mae:1057.81\teval-mae:1133.39\n",
      "[2850]\ttrain-mae:1056.69\teval-mae:1133.24\n",
      "[2900]\ttrain-mae:1055.66\teval-mae:1133.11\n",
      "[2950]\ttrain-mae:1054.55\teval-mae:1133\n",
      "[3000]\ttrain-mae:1053.39\teval-mae:1132.88\n",
      "[3050]\ttrain-mae:1052.35\teval-mae:1132.73\n",
      "[3100]\ttrain-mae:1051.28\teval-mae:1132.6\n",
      "[3150]\ttrain-mae:1050.29\teval-mae:1132.45\n",
      "[3200]\ttrain-mae:1049.27\teval-mae:1132.35\n",
      "[3250]\ttrain-mae:1048.22\teval-mae:1132.28\n",
      "[3300]\ttrain-mae:1047.18\teval-mae:1132.19\n",
      "[3350]\ttrain-mae:1045.97\teval-mae:1132.1\n",
      "[3400]\ttrain-mae:1044.99\teval-mae:1132.03\n",
      "[3450]\ttrain-mae:1044.02\teval-mae:1131.87\n",
      "[3500]\ttrain-mae:1042.94\teval-mae:1131.74\n",
      "[3550]\ttrain-mae:1041.81\teval-mae:1131.69\n",
      "[3600]\ttrain-mae:1040.87\teval-mae:1131.59\n",
      "[3650]\ttrain-mae:1039.91\teval-mae:1131.5\n",
      "[3700]\ttrain-mae:1038.88\teval-mae:1131.45\n",
      "[3750]\ttrain-mae:1037.9\teval-mae:1131.41\n",
      "[3800]\ttrain-mae:1036.87\teval-mae:1131.37\n",
      "[3850]\ttrain-mae:1035.93\teval-mae:1131.38\n",
      "[3900]\ttrain-mae:1034.88\teval-mae:1131.3\n",
      "[3950]\ttrain-mae:1033.86\teval-mae:1131.23\n",
      "[4000]\ttrain-mae:1032.8\teval-mae:1131.15\n",
      "[4050]\ttrain-mae:1031.92\teval-mae:1131.07\n",
      "[4100]\ttrain-mae:1030.87\teval-mae:1131.05\n",
      "[4150]\ttrain-mae:1029.84\teval-mae:1131\n",
      "[4200]\ttrain-mae:1028.82\teval-mae:1130.96\n",
      "[4250]\ttrain-mae:1027.81\teval-mae:1130.91\n",
      "[4300]\ttrain-mae:1026.89\teval-mae:1130.9\n",
      "Stopping. Best iteration:\n",
      "[4272]\ttrain-mae:1027.45\teval-mae:1130.87\n",
      "\n",
      "eval-MAE: 1130.867326\n",
      "\n",
      " Fold 9\n",
      "[0]\ttrain-mae:3235.36\teval-mae:3230.23\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2781.19\teval-mae:2775.96\n",
      "[100]\ttrain-mae:2232.57\teval-mae:2228.44\n",
      "[150]\ttrain-mae:1859.72\teval-mae:1858.44\n",
      "[200]\ttrain-mae:1613.3\teval-mae:1614.75\n",
      "[250]\ttrain-mae:1450.37\teval-mae:1454.69\n",
      "[300]\ttrain-mae:1342.7\teval-mae:1349.26\n",
      "[350]\ttrain-mae:1271.27\teval-mae:1280.37\n",
      "[400]\ttrain-mae:1223.8\teval-mae:1235.07\n",
      "[450]\ttrain-mae:1192.05\teval-mae:1204.89\n",
      "[500]\ttrain-mae:1170.18\teval-mae:1184.8\n",
      "[550]\ttrain-mae:1155.24\teval-mae:1171.17\n",
      "[600]\ttrain-mae:1144.45\teval-mae:1161.67\n",
      "[650]\ttrain-mae:1136.53\teval-mae:1154.97\n",
      "[700]\ttrain-mae:1130.57\teval-mae:1150.1\n",
      "[750]\ttrain-mae:1125.61\teval-mae:1146.3\n",
      "[800]\ttrain-mae:1121.52\teval-mae:1143.43\n",
      "[850]\ttrain-mae:1118.14\teval-mae:1141.12\n",
      "[900]\ttrain-mae:1115.3\teval-mae:1139.27\n",
      "[950]\ttrain-mae:1112.5\teval-mae:1137.6\n",
      "[1000]\ttrain-mae:1110.19\teval-mae:1136.34\n",
      "[1050]\ttrain-mae:1107.78\teval-mae:1135.16\n",
      "[1100]\ttrain-mae:1105.71\teval-mae:1134.2\n",
      "[1150]\ttrain-mae:1103.78\teval-mae:1133.35\n",
      "[1200]\ttrain-mae:1101.97\teval-mae:1132.61\n",
      "[1250]\ttrain-mae:1100.06\teval-mae:1131.88\n",
      "[1300]\ttrain-mae:1098.44\teval-mae:1131.23\n",
      "[1350]\ttrain-mae:1096.99\teval-mae:1130.64\n",
      "[1400]\ttrain-mae:1095.32\teval-mae:1130.09\n",
      "[1450]\ttrain-mae:1093.62\teval-mae:1129.47\n",
      "[1500]\ttrain-mae:1092.07\teval-mae:1128.99\n",
      "[1550]\ttrain-mae:1090.64\teval-mae:1128.53\n",
      "[1600]\ttrain-mae:1089.19\teval-mae:1128.07\n",
      "[1650]\ttrain-mae:1087.7\teval-mae:1127.66\n",
      "[1700]\ttrain-mae:1086.32\teval-mae:1127.16\n",
      "[1750]\ttrain-mae:1085.02\teval-mae:1126.83\n",
      "[1800]\ttrain-mae:1083.6\teval-mae:1126.46\n",
      "[1850]\ttrain-mae:1082.32\teval-mae:1126.17\n",
      "[1900]\ttrain-mae:1081.04\teval-mae:1125.9\n",
      "[1950]\ttrain-mae:1079.76\teval-mae:1125.62\n",
      "[2000]\ttrain-mae:1078.49\teval-mae:1125.4\n",
      "[2050]\ttrain-mae:1077.19\teval-mae:1125.17\n",
      "[2100]\ttrain-mae:1075.96\teval-mae:1124.85\n",
      "[2150]\ttrain-mae:1074.64\teval-mae:1124.64\n",
      "[2200]\ttrain-mae:1073.54\teval-mae:1124.47\n",
      "[2250]\ttrain-mae:1072.23\teval-mae:1124.17\n",
      "[2300]\ttrain-mae:1071.07\teval-mae:1123.88\n",
      "[2350]\ttrain-mae:1069.86\teval-mae:1123.63\n",
      "[2400]\ttrain-mae:1068.75\teval-mae:1123.41\n",
      "[2450]\ttrain-mae:1067.7\teval-mae:1123.21\n",
      "[2500]\ttrain-mae:1066.46\teval-mae:1123.05\n",
      "[2550]\ttrain-mae:1065.3\teval-mae:1122.9\n",
      "[2600]\ttrain-mae:1064.13\teval-mae:1122.78\n",
      "[2650]\ttrain-mae:1063.01\teval-mae:1122.56\n",
      "[2700]\ttrain-mae:1061.9\teval-mae:1122.33\n",
      "[2750]\ttrain-mae:1060.79\teval-mae:1122.17\n",
      "[2800]\ttrain-mae:1059.62\teval-mae:1122\n",
      "[2850]\ttrain-mae:1058.51\teval-mae:1121.81\n",
      "[2900]\ttrain-mae:1057.51\teval-mae:1121.71\n",
      "[2950]\ttrain-mae:1056.4\teval-mae:1121.59\n",
      "[3000]\ttrain-mae:1055.43\teval-mae:1121.47\n",
      "[3050]\ttrain-mae:1054.28\teval-mae:1121.41\n",
      "[3100]\ttrain-mae:1053.18\teval-mae:1121.25\n",
      "[3150]\ttrain-mae:1052.15\teval-mae:1121.16\n",
      "[3200]\ttrain-mae:1050.96\teval-mae:1121.01\n",
      "[3250]\ttrain-mae:1049.8\teval-mae:1120.84\n",
      "[3300]\ttrain-mae:1048.67\teval-mae:1120.76\n",
      "[3350]\ttrain-mae:1047.64\teval-mae:1120.65\n",
      "[3400]\ttrain-mae:1046.64\teval-mae:1120.5\n",
      "[3450]\ttrain-mae:1045.58\teval-mae:1120.42\n",
      "[3500]\ttrain-mae:1044.54\teval-mae:1120.24\n",
      "[3550]\ttrain-mae:1043.56\teval-mae:1120.19\n",
      "Stopping. Best iteration:\n",
      "[3529]\ttrain-mae:1044\teval-mae:1120.15\n",
      "\n",
      "eval-MAE: 1120.146468\n",
      "\n",
      " Fold 10\n",
      "[0]\ttrain-mae:3236.76\teval-mae:3217.64\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mae:2782.02\teval-mae:2763.17\n",
      "[100]\ttrain-mae:2232.68\teval-mae:2217.38\n",
      "[150]\ttrain-mae:1860.14\teval-mae:1847.49\n",
      "[200]\ttrain-mae:1613.8\teval-mae:1603.15\n",
      "[250]\ttrain-mae:1451\teval-mae:1442.73\n",
      "[300]\ttrain-mae:1343.25\teval-mae:1337.61\n",
      "[350]\ttrain-mae:1271.85\teval-mae:1268.92\n",
      "[400]\ttrain-mae:1224.32\teval-mae:1224.36\n",
      "[450]\ttrain-mae:1192.38\teval-mae:1195.95\n",
      "[500]\ttrain-mae:1170.75\teval-mae:1177.65\n",
      "[550]\ttrain-mae:1155.6\teval-mae:1165.38\n",
      "[600]\ttrain-mae:1144.68\teval-mae:1157.07\n",
      "[650]\ttrain-mae:1136.78\teval-mae:1151.29\n",
      "[700]\ttrain-mae:1130.66\teval-mae:1147.02\n",
      "[750]\ttrain-mae:1125.84\teval-mae:1143.9\n",
      "[800]\ttrain-mae:1121.78\teval-mae:1141.52\n",
      "[850]\ttrain-mae:1118.34\teval-mae:1139.64\n",
      "[900]\ttrain-mae:1115.4\teval-mae:1138.03\n",
      "[950]\ttrain-mae:1112.77\teval-mae:1136.7\n",
      "[1000]\ttrain-mae:1110.6\teval-mae:1135.61\n",
      "[1050]\ttrain-mae:1108.25\teval-mae:1134.5\n",
      "[1100]\ttrain-mae:1106.28\teval-mae:1133.65\n",
      "[1150]\ttrain-mae:1104.42\teval-mae:1132.92\n",
      "[1200]\ttrain-mae:1102.65\teval-mae:1132.19\n",
      "[1250]\ttrain-mae:1100.77\teval-mae:1131.48\n",
      "[1300]\ttrain-mae:1099.05\teval-mae:1130.84\n",
      "[1350]\ttrain-mae:1097.44\teval-mae:1130.37\n",
      "[1400]\ttrain-mae:1095.68\teval-mae:1129.79\n",
      "[1450]\ttrain-mae:1094.15\teval-mae:1129.4\n",
      "[1500]\ttrain-mae:1092.68\teval-mae:1128.99\n",
      "[1550]\ttrain-mae:1091.12\teval-mae:1128.58\n",
      "[1600]\ttrain-mae:1089.67\teval-mae:1128.18\n",
      "[1650]\ttrain-mae:1088.21\teval-mae:1127.92\n",
      "[1700]\ttrain-mae:1086.85\teval-mae:1127.57\n",
      "[1750]\ttrain-mae:1085.49\teval-mae:1127.14\n",
      "[1800]\ttrain-mae:1084.04\teval-mae:1126.85\n",
      "[1850]\ttrain-mae:1082.74\teval-mae:1126.65\n",
      "[1900]\ttrain-mae:1081.47\teval-mae:1126.3\n",
      "[1950]\ttrain-mae:1080.17\teval-mae:1126.03\n",
      "[2000]\ttrain-mae:1078.81\teval-mae:1125.89\n",
      "[2050]\ttrain-mae:1077.53\teval-mae:1125.69\n",
      "[2100]\ttrain-mae:1076.37\teval-mae:1125.47\n",
      "[2150]\ttrain-mae:1075.01\teval-mae:1125.27\n",
      "[2200]\ttrain-mae:1073.84\teval-mae:1124.99\n",
      "[2250]\ttrain-mae:1072.58\teval-mae:1124.69\n",
      "[2300]\ttrain-mae:1071.37\teval-mae:1124.42\n",
      "[2350]\ttrain-mae:1070.13\teval-mae:1124.23\n",
      "[2400]\ttrain-mae:1068.94\teval-mae:1124.2\n",
      "[2450]\ttrain-mae:1067.88\teval-mae:1123.96\n",
      "[2500]\ttrain-mae:1066.63\teval-mae:1123.7\n",
      "[2550]\ttrain-mae:1065.41\teval-mae:1123.53\n",
      "[2600]\ttrain-mae:1064.24\teval-mae:1123.43\n",
      "[2650]\ttrain-mae:1063.14\teval-mae:1123.46\n",
      "[2700]\ttrain-mae:1061.98\teval-mae:1123.18\n",
      "[2750]\ttrain-mae:1060.83\teval-mae:1123.15\n",
      "[2800]\ttrain-mae:1059.61\teval-mae:1123.09\n",
      "[2850]\ttrain-mae:1058.42\teval-mae:1123\n",
      "[2900]\ttrain-mae:1057.38\teval-mae:1122.96\n",
      "[2950]\ttrain-mae:1056.28\teval-mae:1122.89\n",
      "[3000]\ttrain-mae:1055.29\teval-mae:1122.9\n",
      "Stopping. Best iteration:\n",
      "[2971]\ttrain-mae:1055.88\teval-mae:1122.83\n",
      "\n",
      "eval-MAE: 1122.825483\n",
      "Average eval-MAE: 1127.333908\n",
      "Writing results\n",
      "10-fold average prediction:\n",
      "Writing submission: xgbmodel_d1127.333908_2016-12-10-05-15.csv\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\n Fold %d' % (i+1))\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[test_index]\n",
    "    y_train, y_val = train_y.iloc[train_index], train_y.iloc[test_index]\n",
    "\n",
    "    rand_state = 2016\n",
    "\n",
    "    params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.5,\n",
    "        #'silent': 1,\n",
    "        'subsample': 0.5,\n",
    "        'learning_rate': 0.005,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 12,\n",
    "        'min_child_weight': 100,\n",
    "        'booster': 'gbtree',\n",
    "          'nthread':10,\n",
    "        'lambda':0.01,\n",
    "        \n",
    "        'scale_pos_weight':1.0\n",
    "    }\n",
    "\n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_valid = xgb.DMatrix(X_val, label=y_val)\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "\n",
    "    clf = xgb.train(params,\n",
    "                    d_train,\n",
    "                    10000,\n",
    "                    watchlist,\n",
    "                    early_stopping_rounds=50,\n",
    "                    obj=fair_obj,\n",
    "                    feval=xg_eval_mae,\n",
    "                    verbose_eval=50)\n",
    "\n",
    "    xgb_rounds.append(clf.best_iteration)\n",
    "    scores_val = clf.predict(d_valid, ntree_limit=clf.best_ntree_limit)\n",
    "    cv_score = mean_absolute_error(np.exp(y_val), np.exp(scores_val))\n",
    "    print('eval-MAE: %.6f' % cv_score)\n",
    "    y_pred = np.exp(clf.predict(d_test, ntree_limit=clf.best_ntree_limit)) - shift\n",
    "    \n",
    "    temp = pd.DataFrame({'id':ids,'result':y_pred})\n",
    "    temp.to_csv(\"xgb_pred_fold_\"+str(i)+\".csv\",index=False)\n",
    "    if i > 0:\n",
    "        fpred = pred + y_pred\n",
    "    else:\n",
    "        fpred = y_pred\n",
    "    pred = fpred\n",
    "    cv_sum = cv_sum + cv_score\n",
    "\n",
    "mpred = pred / n_folds\n",
    "score = cv_sum / n_folds\n",
    "print('Average eval-MAE: %.6f' % score)\n",
    "n_rounds = int(np.mean(xgb_rounds))\n",
    "\n",
    "print(\"Writing results\")\n",
    "result = pd.DataFrame(mpred, columns=['loss'])\n",
    "result[\"id\"] = ids\n",
    "result = result.set_index(\"id\")\n",
    "print(\"%d-fold average prediction:\" % n_folds)\n",
    "\n",
    "now = datetime.now()\n",
    "score = str(round((cv_sum / n_folds), 6))\n",
    "sub_file = 'xgbmodel_d' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "print(\"Writing submission: %s\" % sub_file)\n",
    "result.to_csv(sub_file, index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
