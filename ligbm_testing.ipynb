{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/udit/ipython/notebook/all/input\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "\n",
    "shift = 200\n",
    "COMB_FEATURE = 'cat80,cat87,cat57,cat12,cat79,cat10,cat7,cat89,cat2,cat72,' \\\n",
    "               'cat81,cat11,cat1,cat13,cat9,cat3,cat16,cat90,cat23,cat36,' \\\n",
    "               'cat73,cat103,cat40,cat28,cat111,cat6,cat76,cat50,cat5,' \\\n",
    "               'cat4,cat14,cat38,cat24,cat82,cat25'.split(',')\n",
    "\n",
    "def encode(charcode):\n",
    "    r = 0\n",
    "    ln = len(str(charcode))\n",
    "    for i in range(ln):\n",
    "        r += (ord(str(charcode)[i]) - ord('A') + 1) * 26 ** (ln - i - 1)\n",
    "    return r\n",
    "\n",
    "fair_constant = 0.7\n",
    "def fair_obj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    x = (preds - labels)\n",
    "    den = abs(x) + fair_constant\n",
    "    grad = fair_constant * x / (den)\n",
    "    hess = fair_constant * fair_constant / (den * den)\n",
    "    return grad, hess\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y)-shift,\n",
    "                                      np.exp(yhat)-shift)\n",
    "def mungeskewed(train, test, numeric_feats):\n",
    "    ntrain = train.shape[0]\n",
    "    test['loss'] = 0\n",
    "    train_test = pd.concat((train, test)).reset_index(drop=True)\n",
    "    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.25]\n",
    "    skewed_feats = skewed_feats.index\n",
    "\n",
    "    for feats in skewed_feats:\n",
    "        train_test[feats] = train_test[feats] + 1\n",
    "        train_test[feats], lam = boxcox(train_test[feats])\n",
    "    return train_test, ntrain\n",
    "\n",
    "print('\\nStarted')\n",
    "directory = '../input/'\n",
    "train = pd.read_csv(directory + 'train.csv')\n",
    "test = pd.read_csv(directory + 'test.csv')\n",
    "\n",
    "numeric_feats = [x for x in train.columns[1:-1] if 'cont' in x]\n",
    "categorical_feats = [x for x in train.columns[1:-1] if 'cat' in x]\n",
    "train_test, ntrain = mungeskewed(train, test, numeric_feats)\n",
    "\n",
    "# taken from Vladimir's script (https://www.kaggle.com/iglovikov/allstate-claims-severity/xgb-1114)\n",
    "for column in list(train.select_dtypes(include=['object']).columns):\n",
    "    if train[column].nunique() != test[column].nunique():\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        remove_train = set_train - set_test\n",
    "        remove_test = set_test - set_train\n",
    "\n",
    "        remove = remove_train.union(remove_test)\n",
    "\n",
    "\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "\n",
    "\n",
    "        train_test[column] = train_test[column].apply(lambda x: filter_cat(x), 1)\n",
    "\n",
    "# taken from Ali's script (https://www.kaggle.com/aliajouz/allstate-claims-severity/singel-model-lb-1117)\n",
    "train_test[\"cont1\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont1\"]))\n",
    "train_test[\"cont4\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont4\"]))\n",
    "train_test[\"cont5\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont5\"]))\n",
    "train_test[\"cont8\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont8\"]))\n",
    "train_test[\"cont10\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont10\"]))\n",
    "train_test[\"cont11\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont11\"]))\n",
    "train_test[\"cont12\"] = np.sqrt(preprocessing.minmax_scale(train_test[\"cont12\"]))\n",
    "\n",
    "train_test[\"cont6\"] = np.log(preprocessing.minmax_scale(train_test[\"cont6\"]) + 0000.1)\n",
    "train_test[\"cont7\"] = np.log(preprocessing.minmax_scale(train_test[\"cont7\"]) + 0000.1)\n",
    "train_test[\"cont9\"] = np.log(preprocessing.minmax_scale(train_test[\"cont9\"]) + 0000.1)\n",
    "train_test[\"cont13\"] = np.log(preprocessing.minmax_scale(train_test[\"cont13\"]) + 0000.1)\n",
    "train_test[\"cont14\"] = (np.maximum(train_test[\"cont14\"] - 0.179722, 0) / 0.665122) ** 0.25\n",
    "\n",
    "print('')\n",
    "for comb in itertools.combinations(COMB_FEATURE, 2):\n",
    "    feat = comb[0] + \"_\" + comb[1]\n",
    "    train_test[feat] = train_test[comb[0]] + train_test[comb[1]]\n",
    "    train_test[feat] = train_test[feat].apply(encode)\n",
    "    print('Combining Columns:', feat)\n",
    "\n",
    "print('')\n",
    "for col in categorical_feats:\n",
    "    print('Analyzing Column:', col)\n",
    "    train_test[col] = train_test[col].apply(encode)\n",
    "\n",
    "print(train_test[categorical_feats])\n",
    "\n",
    "ss = StandardScaler()\n",
    "train_test[numeric_feats] = \\\n",
    "    ss.fit_transform(train_test[numeric_feats].values)\n",
    "\n",
    "train = train_test.iloc[:ntrain, :].copy()\n",
    "test = train_test.iloc[ntrain:, :].copy()\n",
    "\n",
    "print('\\nMedian Loss:', train.loss.median())\n",
    "print('Mean Loss:', train.loss.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"train_processed_v2.csv\",index=False)\n",
    "test.to_csv(\"train_processed_v2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = pd.read_csv('../input/test.csv')['id']\n",
    "train_y = np.log(train['loss'] + shift)\n",
    "train_x = train.drop(['loss','id'], axis=1)\n",
    "test_x = test.drop(['loss','id'], axis=1)\n",
    "\n",
    "n_folds = 10\n",
    "cv_sum = 0\n",
    "early_stopping = 100\n",
    "fpred = []\n",
    "xgb_rounds = []\n",
    "\n",
    "#d_train_full = xgb.DMatrix(train_x, label=train_y)\n",
    "#d_test = xgb.DMatrix(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y=train_y[(train['loss']>389) &(train['loss']<13981)]\n",
    "train_x=train_x[(train['loss']>389) &(train['loss']<13981)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1\n",
      "((166100, 725), (18456, 725))\n",
      "training fold\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pylightgbm.models import GBMRegressor\n",
    "os.environ['LIGHTGBM_EXEC'] = \"/home/udit/LightGBM/lightgbm\"\n",
    "gbmlib=\"/home/udit/LightGBM/lightgbm\"\n",
    "#https://github.com/ArdalanM/pyLightGBM\n",
    "kf = KFold(train_x.shape[0], n_folds=n_folds)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\n Fold %d' % (i+1))\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[test_index]\n",
    "    y_train, y_val = train_y.iloc[train_index], train_y.iloc[test_index]\n",
    "    print(X_train.shape,X_val.shape)\n",
    "    model = GBMRegressor(\n",
    "            #exec_path=gbmlib,\n",
    "            num_threads=8,\n",
    "            boosting_type='gbdt',\n",
    "            learning_rate = 0.003,\n",
    "            num_iterations = 10000, \n",
    "            tree_learner=\"feature\",\n",
    "            metric='l1',\n",
    "            bagging_freq=5,\n",
    "            #max_depth=8,\n",
    "            #lambda_l2=gamma,\n",
    "            min_data_in_leaf=8,\n",
    "            verbose = False, \n",
    "            early_stopping_round = 10,\n",
    "            feature_fraction = 0.5,\n",
    "            bagging_fraction = 0.5,\n",
    "            min_sum_hessian_in_leaf=100\n",
    "           )   \n",
    "    print('training fold')\n",
    "    model.fit(\n",
    "        X_train.values, \n",
    "        y_train, \n",
    "        test_data = [(X_val.values, y_val )]\n",
    "        )\n",
    "    print(\"training done\")\n",
    "    scores_val = model.predict(X_val.values)\n",
    "    print(\"validation prediction done\")\n",
    "    cv_score = mean_absolute_error(np.exp(y_val), np.exp(scores_val))\n",
    "    y_pred = np.exp(model.predict(test_x.values)) - shift\n",
    "    print(\"test prediction done\")\n",
    "    if i > 0:\n",
    "        fpred = pred + y_pred\n",
    "    else:\n",
    "        fpred = y_pred\n",
    "    pred = fpred\n",
    "    cv_sum = cv_sum + cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average eval-MAE: 4691.342413\n"
     ]
    }
   ],
   "source": [
    "mpred = pred / n_folds\n",
    "score = cv_sum / n_folds\n",
    "print('Average eval-MAE: %.6f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results\n",
      "10-fold average prediction:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4691.342413'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Writing results\")\n",
    "result = pd.DataFrame(mpred, columns=['loss'])\n",
    "result[\"id\"] = ids\n",
    "result = result.set_index(\"id\")\n",
    "print(\"%d-fold average prediction:\" % n_folds)\n",
    "now = datetime.now()\n",
    "score = str(round((cv_sum / n_folds), 6))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission: output/submission_5fold-average-xgb_fairobj_4691.342413_2016-12-01-19-37.csv\n"
     ]
    }
   ],
   "source": [
    "sub_file = 'output/submission_5fold-average-xgb_fairobj_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "print(\"Writing submission: %s\" % sub_file)\n",
    "result.to_csv(sub_file, index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(train.shape[0], n_folds=n_folds)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print('\\n Fold %d' % (i+1))\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[test_index]\n",
    "    y_train, y_val = train_y.iloc[train_index], train_y.iloc[test_index]\n",
    "\n",
    "    rand_state = 2016\n",
    "\n",
    "    params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.03,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 12,\n",
    "        'min_child_weight': 100,\n",
    "        'booster': 'gbtree'}\n",
    "\n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_valid = xgb.DMatrix(X_val, label=y_val)\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "\n",
    "    clf = xgb.train(params,\n",
    "                    d_train,\n",
    "                    100000,\n",
    "                    watchlist,\n",
    "                    early_stopping_rounds=50,\n",
    "                    obj=fair_obj,\n",
    "                    feval=xg_eval_mae)\n",
    "\n",
    "    xgb_rounds.append(clf.best_iteration)\n",
    "    scores_val = clf.predict(d_valid, ntree_limit=clf.best_ntree_limit)\n",
    "    cv_score = mean_absolute_error(np.exp(y_val), np.exp(scores_val))\n",
    "    print('eval-MAE: %.6f' % cv_score)\n",
    "    y_pred = np.exp(clf.predict(d_test, ntree_limit=clf.best_ntree_limit)) - shift\n",
    "\n",
    "    if i > 0:\n",
    "        fpred = pred + y_pred\n",
    "    else:\n",
    "        fpred = y_pred\n",
    "    pred = fpred\n",
    "    cv_sum = cv_sum + cv_score\n",
    "\n",
    "mpred = pred / n_folds\n",
    "score = cv_sum / n_folds\n",
    "print('Average eval-MAE: %.6f' % score)\n",
    "n_rounds = int(np.mean(xgb_rounds))\n",
    "\n",
    "print(\"Writing results\")\n",
    "result = pd.DataFrame(mpred, columns=['loss'])\n",
    "result[\"id\"] = ids\n",
    "result = result.set_index(\"id\")\n",
    "print(\"%d-fold average prediction:\" % n_folds)\n",
    "\n",
    "now = datetime.now()\n",
    "score = str(round((cv_sum / n_folds), 6))\n",
    "sub_file = 'output/submission_5fold-average-xgb_fairobj_' + str(score) + '_' + str(\n",
    "    now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "print(\"Writing submission: %s\" % sub_file)\n",
    "result.to_csv(sub_file, index=True, index_label='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
